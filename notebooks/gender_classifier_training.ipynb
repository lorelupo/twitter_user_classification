{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab0aa90-74e5-4810-ad86-d62b738839e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc8c712-0765-4adc-b32c-cae0f7c0eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline, set_seed\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2047646a-e7ce-44d7-bd59-92cae2e040f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "# SEED = 1\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.manual_seed(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8436723-322d-4fdf-99ea-143b5438eaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53404b9-bca8-4635-ad7e-579a65744b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f16b52b-23dd-448a-aaeb-3cc95a4ca6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/g100/home/userexternal/mhabibi0/'\n",
    "work_dir = '/g100_work/IscrC_mental'\n",
    "\n",
    "hdata_dir = os.path.join(home_dir, 'Data')\n",
    "wdata_dir = os.path.join(work_dir, 'data')\n",
    "uc_dir = os.path.join(wdata_dir, 'user_classification')\n",
    "model_dir = os.path.join(home_dir, 'Models', 'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63c6498-d992-45d5-9484-42a018b70f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user age data\n",
    "path  = os.path.join(uc_dir, 'data_for_models_train.pkl')\n",
    "df = pd.read_pickle(path)\n",
    "df['male'] = df['is_male'].astype(int)\n",
    "df['text']  = 'bio: ' + df['masked_bio'] + '. ' + 'tweets: ' + df['long_text'] \n",
    "df['text'] = df['text'].str.replace('\\r|\\n', ' ', regex=True)\n",
    "df_train, df_valid = train_test_split(df[['user_id', 'text', 'male']], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b903ba3-9c33-4489-9b4d-a5aaaf587dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['text'].values\n",
    "y_train = df_train['male'].values\n",
    "\n",
    "X_valid = df_valid['text'].values\n",
    "y_valid = df_valid['male'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac98f8c-9155-43f2-983c-0014b57807c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get the predicted class labels\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions)\n",
    "        recall = recall_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "        metrics = {'accuracy': accuracy,  'f1': f1 }\n",
    "\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d228767-2023-4a68-9d93-09a1dc4ec57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a46ec20-c9b9-43fd-9a6b-3ad286e59e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_tokenize(X_text, tokenizer, max_length=512, batch_size=64):\n",
    "\n",
    "    # Dictionary to hold tokenized batches\n",
    "    encodings = {}\n",
    "\n",
    "    # Calculate the number of batches needed\n",
    "    num_batches = len(X_text) // batch_size + int(len(X_text) % batch_size > 0)\n",
    "\n",
    "    # Iterate over the data in batches\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min(len(X_text), (i + 1) * batch_size)\n",
    "\n",
    "        # Tokenize the current batch of texts\n",
    "        batch_encodings = tokenizer.batch_encode_plus(\n",
    "            list(X_text[batch_start:batch_end]),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        # Merge the batch tokenizations into the main dictionary\n",
    "        for key, val in batch_encodings.items():\n",
    "            if key not in encodings:\n",
    "                encodings[key] = []\n",
    "            encodings[key].extend(val)\n",
    "\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05948e2-ea65-4fbe-b3b4-19aef45f407d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Twitter XLM Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1bbfb8a-fa5b-4cae-a7b0-39df5a297ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base\")\n",
    "\n",
    "train_encodings = batch_tokenize(X_train, tokenizer)\n",
    "valid_encodings = batch_tokenize(X_valid, tokenizer)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, y_train)\n",
    "valid_dataset = TweetDataset(valid_encodings, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e42c8aa-1c0f-4d03-9241-6a448b488b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9d6cd-5fc0-4b22-81d6-461237a15e44",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### XLm Roberta + freezing + early stopping + linear schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a9e363-ca17-4710-86b7-cb603b3f6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(DEVICE)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "LR = 2e-5\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Freeze all layers except the classifier for the first few epochs\n",
    "freeze_steps = 2\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name: # classifier layer\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# Initialize best accuracy and epochs since improvement\n",
    "best_f1 = 0.0\n",
    "epochs_since_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fa8cfd4-fbf6-4197-9c6a-13da4398c55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0100 | Batch 0000/0540 | Loss: 0.6945\n",
      "Epoch: 0001/0100 | Batch 0050/0540 | Loss: 0.6478\n",
      "Epoch: 0001/0100 | Batch 0100/0540 | Loss: 0.6338\n",
      "Epoch: 0001/0100 | Batch 0150/0540 | Loss: 0.7492\n",
      "Epoch: 0001/0100 | Batch 0200/0540 | Loss: 0.6664\n",
      "Epoch: 0001/0100 | Batch 0250/0540 | Loss: 0.6279\n",
      "Epoch: 0001/0100 | Batch 0300/0540 | Loss: 0.6998\n",
      "Epoch: 0001/0100 | Batch 0350/0540 | Loss: 0.7307\n",
      "Epoch: 0001/0100 | Batch 0400/0540 | Loss: 0.6923\n",
      "Epoch: 0001/0100 | Batch 0450/0540 | Loss: 0.5835\n",
      "Epoch: 0001/0100 | Batch 0500/0540 | Loss: 0.6340\n",
      "Training metrics: {'accuracy': 0.6279513888888889, 'f1': 0.7714458388140353}%\n",
      "Valid metrics: {'accuracy': 0.6239583333333333, 'f1': 0.7684413085311097}%\n",
      "Time elapsed: 6.67 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002/0100 | Batch 0000/0540 | Loss: 0.6620\n",
      "Epoch: 0002/0100 | Batch 0050/0540 | Loss: 0.6861\n",
      "Epoch: 0002/0100 | Batch 0100/0540 | Loss: 0.7286\n",
      "Epoch: 0002/0100 | Batch 0150/0540 | Loss: 0.6604\n",
      "Epoch: 0002/0100 | Batch 0200/0540 | Loss: 0.6608\n",
      "Epoch: 0002/0100 | Batch 0250/0540 | Loss: 0.6044\n",
      "Epoch: 0002/0100 | Batch 0300/0540 | Loss: 0.6467\n",
      "Epoch: 0002/0100 | Batch 0350/0540 | Loss: 0.6007\n",
      "Epoch: 0002/0100 | Batch 0400/0540 | Loss: 0.7214\n",
      "Epoch: 0002/0100 | Batch 0450/0540 | Loss: 0.6424\n",
      "Epoch: 0002/0100 | Batch 0500/0540 | Loss: 0.6268\n",
      "Training metrics: {'accuracy': 0.6329282407407407, 'f1': 0.7727907726474907}%\n",
      "Valid metrics: {'accuracy': 0.6317708333333333, 'f1': 0.7712714331931413}%\n",
      "Time elapsed: 13.04 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0003/0100 | Batch 0000/0540 | Loss: 0.5385\n",
      "Epoch: 0003/0100 | Batch 0050/0540 | Loss: 0.6333\n",
      "Epoch: 0003/0100 | Batch 0100/0540 | Loss: 0.6467\n",
      "Epoch: 0003/0100 | Batch 0150/0540 | Loss: 0.6048\n",
      "Epoch: 0003/0100 | Batch 0200/0540 | Loss: 0.4794\n",
      "Epoch: 0003/0100 | Batch 0250/0540 | Loss: 0.5871\n",
      "Epoch: 0003/0100 | Batch 0300/0540 | Loss: 0.5807\n",
      "Epoch: 0003/0100 | Batch 0350/0540 | Loss: 0.4739\n",
      "Epoch: 0003/0100 | Batch 0400/0540 | Loss: 0.5869\n",
      "Epoch: 0003/0100 | Batch 0450/0540 | Loss: 0.3140\n",
      "Epoch: 0003/0100 | Batch 0500/0540 | Loss: 0.3326\n",
      "Training metrics: {'accuracy': 0.8575231481481481, 'f1': 0.892526628252139}%\n",
      "Valid metrics: {'accuracy': 0.8322916666666667, 'f1': 0.8744149765990639}%\n",
      "Time elapsed: 22.29 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0004/0100 | Batch 0000/0540 | Loss: 0.2827\n",
      "Epoch: 0004/0100 | Batch 0050/0540 | Loss: 0.2252\n",
      "Epoch: 0004/0100 | Batch 0100/0540 | Loss: 0.4072\n",
      "Epoch: 0004/0100 | Batch 0150/0540 | Loss: 0.5517\n",
      "Epoch: 0004/0100 | Batch 0200/0540 | Loss: 0.1471\n",
      "Epoch: 0004/0100 | Batch 0250/0540 | Loss: 0.5093\n",
      "Epoch: 0004/0100 | Batch 0300/0540 | Loss: 0.2729\n",
      "Epoch: 0004/0100 | Batch 0350/0540 | Loss: 0.2886\n",
      "Epoch: 0004/0100 | Batch 0400/0540 | Loss: 0.2462\n",
      "Epoch: 0004/0100 | Batch 0450/0540 | Loss: 0.2729\n",
      "Epoch: 0004/0100 | Batch 0500/0540 | Loss: 0.3290\n",
      "Training metrics: {'accuracy': 0.9111689814814815, 'f1': 0.9306026493060264}%\n",
      "Valid metrics: {'accuracy': 0.8697916666666666, 'f1': 0.8981255093724532}%\n",
      "Time elapsed: 31.55 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005/0100 | Batch 0000/0540 | Loss: 0.3039\n",
      "Epoch: 0005/0100 | Batch 0050/0540 | Loss: 0.1552\n",
      "Epoch: 0005/0100 | Batch 0100/0540 | Loss: 0.1184\n",
      "Epoch: 0005/0100 | Batch 0150/0540 | Loss: 0.2535\n",
      "Epoch: 0005/0100 | Batch 0200/0540 | Loss: 0.3498\n",
      "Epoch: 0005/0100 | Batch 0250/0540 | Loss: 0.3193\n",
      "Epoch: 0005/0100 | Batch 0300/0540 | Loss: 0.3738\n",
      "Epoch: 0005/0100 | Batch 0350/0540 | Loss: 0.3839\n",
      "Epoch: 0005/0100 | Batch 0400/0540 | Loss: 0.6283\n",
      "Epoch: 0005/0100 | Batch 0450/0540 | Loss: 0.4832\n",
      "Epoch: 0005/0100 | Batch 0500/0540 | Loss: 0.0721\n",
      "Training metrics: {'accuracy': 0.937037037037037, 'f1': 0.9507246376811594}%\n",
      "Valid metrics: {'accuracy': 0.8744791666666667, 'f1': 0.9034068136272544}%\n",
      "Time elapsed: 40.79 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0006/0100 | Batch 0000/0540 | Loss: 0.3165\n",
      "Epoch: 0006/0100 | Batch 0050/0540 | Loss: 0.0856\n",
      "Epoch: 0006/0100 | Batch 0100/0540 | Loss: 0.2454\n",
      "Epoch: 0006/0100 | Batch 0150/0540 | Loss: 0.2680\n",
      "Epoch: 0006/0100 | Batch 0200/0540 | Loss: 0.1836\n",
      "Epoch: 0006/0100 | Batch 0250/0540 | Loss: 0.2434\n",
      "Epoch: 0006/0100 | Batch 0300/0540 | Loss: 0.1057\n",
      "Epoch: 0006/0100 | Batch 0350/0540 | Loss: 0.1160\n",
      "Epoch: 0006/0100 | Batch 0400/0540 | Loss: 0.1054\n",
      "Epoch: 0006/0100 | Batch 0450/0540 | Loss: 0.1174\n",
      "Epoch: 0006/0100 | Batch 0500/0540 | Loss: 0.1456\n",
      "Training metrics: {'accuracy': 0.9605324074074074, 'f1': 0.9688840222648052}%\n",
      "Valid metrics: {'accuracy': 0.8776041666666666, 'f1': 0.9045879009338207}%\n",
      "Time elapsed: 51.21 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0007/0100 | Batch 0000/0540 | Loss: 0.2584\n",
      "Epoch: 0007/0100 | Batch 0050/0540 | Loss: 0.2949\n",
      "Epoch: 0007/0100 | Batch 0100/0540 | Loss: 0.1082\n",
      "Epoch: 0007/0100 | Batch 0150/0540 | Loss: 0.2079\n",
      "Epoch: 0007/0100 | Batch 0200/0540 | Loss: 0.0361\n",
      "Epoch: 0007/0100 | Batch 0250/0540 | Loss: 0.2381\n",
      "Epoch: 0007/0100 | Batch 0300/0540 | Loss: 0.0925\n",
      "Epoch: 0007/0100 | Batch 0350/0540 | Loss: 0.0760\n",
      "Epoch: 0007/0100 | Batch 0400/0540 | Loss: 0.0939\n",
      "Epoch: 0007/0100 | Batch 0450/0540 | Loss: 0.1831\n",
      "Epoch: 0007/0100 | Batch 0500/0540 | Loss: 0.3349\n",
      "Training metrics: {'accuracy': 0.9706597222222222, 'f1': 0.9762829208962904}%\n",
      "Valid metrics: {'accuracy': 0.8630208333333333, 'f1': 0.8876548483554036}%\n",
      "Time elapsed: 60.43 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0008/0100 | Batch 0000/0540 | Loss: 0.3770\n",
      "Epoch: 0008/0100 | Batch 0050/0540 | Loss: 0.0071\n",
      "Epoch: 0008/0100 | Batch 0100/0540 | Loss: 0.0620\n",
      "Epoch: 0008/0100 | Batch 0150/0540 | Loss: 0.0521\n",
      "Epoch: 0008/0100 | Batch 0200/0540 | Loss: 0.0796\n",
      "Epoch: 0008/0100 | Batch 0250/0540 | Loss: 0.0817\n",
      "Epoch: 0008/0100 | Batch 0300/0540 | Loss: 0.0209\n",
      "Epoch: 0008/0100 | Batch 0350/0540 | Loss: 0.1196\n",
      "Epoch: 0008/0100 | Batch 0400/0540 | Loss: 0.1203\n",
      "Epoch: 0008/0100 | Batch 0450/0540 | Loss: 0.2236\n",
      "Epoch: 0008/0100 | Batch 0500/0540 | Loss: 0.0767\n",
      "Training metrics: {'accuracy': 0.9844328703703704, 'f1': 0.9875376418809358}%\n",
      "Valid metrics: {'accuracy': 0.8682291666666667, 'f1': 0.8943632567849688}%\n",
      "Time elapsed: 69.65 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0009/0100 | Batch 0000/0540 | Loss: 0.0220\n",
      "Epoch: 0009/0100 | Batch 0050/0540 | Loss: 0.1150\n",
      "Epoch: 0009/0100 | Batch 0100/0540 | Loss: 0.0669\n",
      "Epoch: 0009/0100 | Batch 0150/0540 | Loss: 0.1139\n",
      "Epoch: 0009/0100 | Batch 0200/0540 | Loss: 0.0133\n",
      "Epoch: 0009/0100 | Batch 0250/0540 | Loss: 0.0177\n",
      "Epoch: 0009/0100 | Batch 0300/0540 | Loss: 0.1678\n",
      "Epoch: 0009/0100 | Batch 0350/0540 | Loss: 0.0264\n",
      "Epoch: 0009/0100 | Batch 0400/0540 | Loss: 0.0596\n",
      "Epoch: 0009/0100 | Batch 0450/0540 | Loss: 0.2535\n",
      "Epoch: 0009/0100 | Batch 0500/0540 | Loss: 0.0072\n",
      "Training metrics: {'accuracy': 0.9867476851851852, 'f1': 0.9894852839891639}%\n",
      "Valid metrics: {'accuracy': 0.8697916666666666, 'f1': 0.9004777070063695}%\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    # Unfreeze all layers after few epochs\n",
    "    if epoch == freeze_steps:\n",
    "        model.requires_grad_(True)\n",
    "            \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        ### Logging\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        print(f'Training metrics: '\n",
    "              f'{compute_metrics(model, train_loader, DEVICE)}%'\n",
    "              f'\\nValid metrics: '\n",
    "              f'{compute_metrics(model, valid_loader, DEVICE)}%')\n",
    "\n",
    "        current_f1 = compute_metrics(model, valid_loader, DEVICE)['f1']\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "            # Save the new best model\n",
    "            path = os.path.join(model_dir ,'XLM_gender.pt')\n",
    "            torch.save(model, path)\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_since_improvement >= 3:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93b9a07e-ef7a-48bc-a937-2337764abd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "MODEL = \"dbmdz/bert-base-italian-xxl-cased\"\n",
    "num_labels = len(age_labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)\n",
    "model.to(DEVICE)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "LR = 2e-5\n",
    "\n",
    "# Adam optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Freeze all layers except the classifier for the first few epochs\n",
    "freeze_steps = 2\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name: # classifier layer\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# Initialize best accuracy and epochs since improvement\n",
    "best_accuracy = 0.0\n",
    "epochs_since_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b56b0d8-02df-4a2f-936f-eb7be8db4fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0100 | Batch 0000/0170 | Loss: 1.3928\n",
      "Epoch: 0001/0100 | Batch 0050/0170 | Loss: 1.3165\n",
      "Epoch: 0001/0100 | Batch 0100/0170 | Loss: 1.3359\n",
      "Epoch: 0001/0100 | Batch 0150/0170 | Loss: 1.3451\n",
      "Training metrics: {'accuracy': 0.3507834101382489, 'macro_f1': 0.1875464430350761}%\n",
      "Valid metrics: {'accuracy': 0.3544973544973545, 'macro_f1': 0.19339588251402448}%\n",
      "Time elapsed: 2.18 min\n",
      "Epoch: 0002/0100 | Batch 0000/0170 | Loss: 1.3295\n",
      "Epoch: 0002/0100 | Batch 0050/0170 | Loss: 1.3283\n",
      "Epoch: 0002/0100 | Batch 0100/0170 | Loss: 1.2494\n",
      "Epoch: 0002/0100 | Batch 0150/0170 | Loss: 1.3359\n",
      "Training metrics: {'accuracy': 0.35889400921658987, 'macro_f1': 0.21016802056656747}%\n",
      "Valid metrics: {'accuracy': 0.3637566137566138, 'macro_f1': 0.21248051048822164}%\n",
      "Time elapsed: 4.13 min\n",
      "Epoch: 0003/0100 | Batch 0000/0170 | Loss: 1.2263\n",
      "Epoch: 0003/0100 | Batch 0050/0170 | Loss: 1.3815\n",
      "Epoch: 0003/0100 | Batch 0100/0170 | Loss: 1.3420\n",
      "Epoch: 0003/0100 | Batch 0150/0170 | Loss: 1.0971\n",
      "Training metrics: {'accuracy': 0.4847926267281106, 'macro_f1': 0.3184154367401023}%\n",
      "Valid metrics: {'accuracy': 0.4828042328042328, 'macro_f1': 0.31801613361057124}%\n",
      "Time elapsed: 7.76 min\n",
      "Epoch: 0004/0100 | Batch 0000/0170 | Loss: 1.0121\n",
      "Epoch: 0004/0100 | Batch 0050/0170 | Loss: 1.1451\n",
      "Epoch: 0004/0100 | Batch 0100/0170 | Loss: 1.1644\n",
      "Epoch: 0004/0100 | Batch 0150/0170 | Loss: 1.0945\n",
      "Training metrics: {'accuracy': 0.6388940092165899, 'macro_f1': 0.47174075752006683}%\n",
      "Valid metrics: {'accuracy': 0.5568783068783069, 'macro_f1': 0.37358775654586807}%\n",
      "Time elapsed: 11.38 min\n",
      "Epoch: 0005/0100 | Batch 0000/0170 | Loss: 0.9653\n",
      "Epoch: 0005/0100 | Batch 0050/0170 | Loss: 0.8806\n",
      "Epoch: 0005/0100 | Batch 0100/0170 | Loss: 0.7454\n",
      "Epoch: 0005/0100 | Batch 0150/0170 | Loss: 0.9230\n",
      "Training metrics: {'accuracy': 0.7247926267281106, 'macro_f1': 0.6334329187259959}%\n",
      "Valid metrics: {'accuracy': 0.5634920634920635, 'macro_f1': 0.4275025587845928}%\n",
      "Time elapsed: 15.01 min\n",
      "Epoch: 0006/0100 | Batch 0000/0170 | Loss: 0.5790\n",
      "Epoch: 0006/0100 | Batch 0050/0170 | Loss: 0.6990\n",
      "Epoch: 0006/0100 | Batch 0100/0170 | Loss: 0.5343\n",
      "Epoch: 0006/0100 | Batch 0150/0170 | Loss: 0.7630\n",
      "Training metrics: {'accuracy': 0.8665437788018433, 'macro_f1': 0.8305739616556544}%\n",
      "Valid metrics: {'accuracy': 0.548941798941799, 'macro_f1': 0.45995322401861455}%\n",
      "Time elapsed: 18.62 min\n",
      "Epoch: 0007/0100 | Batch 0000/0170 | Loss: 0.4903\n",
      "Epoch: 0007/0100 | Batch 0050/0170 | Loss: 0.3796\n",
      "Epoch: 0007/0100 | Batch 0100/0170 | Loss: 0.3703\n",
      "Epoch: 0007/0100 | Batch 0150/0170 | Loss: 0.3005\n",
      "Training metrics: {'accuracy': 0.9365898617511521, 'macro_f1': 0.9266089330031569}%\n",
      "Valid metrics: {'accuracy': 0.5198412698412699, 'macro_f1': 0.4502436659689094}%\n",
      "Time elapsed: 22.23 min\n",
      "Epoch: 0008/0100 | Batch 0000/0170 | Loss: 0.3736\n",
      "Epoch: 0008/0100 | Batch 0050/0170 | Loss: 0.3905\n",
      "Epoch: 0008/0100 | Batch 0100/0170 | Loss: 0.1749\n",
      "Epoch: 0008/0100 | Batch 0150/0170 | Loss: 0.1495\n",
      "Training metrics: {'accuracy': 0.9717972350230415, 'macro_f1': 0.9672001875344813}%\n",
      "Valid metrics: {'accuracy': 0.5066137566137566, 'macro_f1': 0.4381143177548939}%\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    # Unfreeze all layers after few epochs\n",
    "    if epoch == freeze_steps:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        ### Logging\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        print(f'Training metrics: '\n",
    "              f'{compute_metrics(model, train_loader, DEVICE)}%'\n",
    "              f'\\nValid metrics: '\n",
    "              f'{compute_metrics(model, valid_loader, DEVICE)}%')\n",
    "\n",
    "        current_accuracy = compute_metrics(model, valid_loader, DEVICE)['accuracy']\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "            # Save the new best model\n",
    "            path = os.path.join(model_dir ,'bert_italian_mod.pt')\n",
    "            torch.save(model, path)\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_since_improvement >= 3:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d92dc8-ba38-4a6c-818f-b65bcb3e00bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "##   bert-tweet-base-italian-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4be1e6d-3e6a-4d37-8fc8-48932b18140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"osiria/bert-tweet-base-italian-uncased\")\n",
    "\n",
    "train_encodings = batch_tokenize(X_train, tokenizer)\n",
    "valid_encodings = batch_tokenize(X_valid, tokenizer)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, y_train)\n",
    "valid_dataset = TweetDataset(valid_encodings, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "410c6eeb-9aa9-4f31-b0b0-db955d8f1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed014920-84f9-4f7d-8043-23ea492ab677",
   "metadata": {},
   "source": [
    "#### Bertweet + freeze + early stopping + linear schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bfa646f-f54c-417a-86e4-e1155939f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at osiria/bert-tweet-base-italian-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at osiria/bert-tweet-base-italian-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "MODEL = \"osiria/bert-tweet-base-italian-uncased\"\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(DEVICE)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "LR = 2e-5\n",
    "\n",
    "# Adam optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Freeze all layers except the classifier for the first few epochs\n",
    "freeze_steps = 2\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name: # classifier layer\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# Initialize best accuracy and epochs since improvement\n",
    "best_f1 = 0.0\n",
    "epochs_since_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f79925c7-9d4f-419a-a71a-938ebc447320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0100 | Batch 0000/0540 | Loss: 0.7959\n",
      "Epoch: 0001/0100 | Batch 0050/0540 | Loss: 0.6919\n",
      "Epoch: 0001/0100 | Batch 0100/0540 | Loss: 0.6879\n",
      "Epoch: 0001/0100 | Batch 0150/0540 | Loss: 0.6742\n",
      "Epoch: 0001/0100 | Batch 0200/0540 | Loss: 0.6685\n",
      "Epoch: 0001/0100 | Batch 0250/0540 | Loss: 0.7083\n",
      "Epoch: 0001/0100 | Batch 0300/0540 | Loss: 0.6494\n",
      "Epoch: 0001/0100 | Batch 0350/0540 | Loss: 0.6638\n",
      "Epoch: 0001/0100 | Batch 0400/0540 | Loss: 0.6609\n",
      "Epoch: 0001/0100 | Batch 0450/0540 | Loss: 0.7390\n",
      "Epoch: 0001/0100 | Batch 0500/0540 | Loss: 0.6568\n",
      "Training metrics: {'accuracy': 0.6261574074074074, 'f1': 0.7698446629613795}%\n",
      "Valid metrics: {'accuracy': 0.6234375, 'f1': 0.7675988428158148}%\n",
      "Time elapsed: 4.91 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002/0100 | Batch 0000/0540 | Loss: 0.6754\n",
      "Epoch: 0002/0100 | Batch 0050/0540 | Loss: 0.6671\n",
      "Epoch: 0002/0100 | Batch 0100/0540 | Loss: 0.5582\n",
      "Epoch: 0002/0100 | Batch 0150/0540 | Loss: 0.6466\n",
      "Epoch: 0002/0100 | Batch 0200/0540 | Loss: 0.8088\n",
      "Epoch: 0002/0100 | Batch 0250/0540 | Loss: 0.6261\n",
      "Epoch: 0002/0100 | Batch 0300/0540 | Loss: 0.6633\n",
      "Epoch: 0002/0100 | Batch 0350/0540 | Loss: 0.7375\n",
      "Epoch: 0002/0100 | Batch 0400/0540 | Loss: 0.6848\n",
      "Epoch: 0002/0100 | Batch 0450/0540 | Loss: 0.6685\n",
      "Epoch: 0002/0100 | Batch 0500/0540 | Loss: 0.7270\n",
      "Training metrics: {'accuracy': 0.6274305555555556, 'f1': 0.7709711846318036}%\n",
      "Valid metrics: {'accuracy': 0.6239583333333333, 'f1': 0.7682926829268294}%\n",
      "Time elapsed: 9.81 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0003/0100 | Batch 0000/0540 | Loss: 0.5864\n",
      "Epoch: 0003/0100 | Batch 0050/0540 | Loss: 0.6693\n",
      "Epoch: 0003/0100 | Batch 0100/0540 | Loss: 0.6063\n",
      "Epoch: 0003/0100 | Batch 0150/0540 | Loss: 0.6274\n",
      "Epoch: 0003/0100 | Batch 0200/0540 | Loss: 0.7238\n",
      "Epoch: 0003/0100 | Batch 0250/0540 | Loss: 0.7661\n",
      "Epoch: 0003/0100 | Batch 0300/0540 | Loss: 0.6638\n",
      "Epoch: 0003/0100 | Batch 0350/0540 | Loss: 0.6677\n",
      "Epoch: 0003/0100 | Batch 0400/0540 | Loss: 0.7035\n",
      "Epoch: 0003/0100 | Batch 0450/0540 | Loss: 0.6897\n",
      "Epoch: 0003/0100 | Batch 0500/0540 | Loss: 0.6378\n",
      "Training metrics: {'accuracy': 0.6280671296296296, 'f1': 0.771533184031851}%\n",
      "Valid metrics: {'accuracy': 0.6244791666666667, 'f1': 0.7688361654376403}%\n",
      "Time elapsed: 17.71 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0004/0100 | Batch 0000/0540 | Loss: 0.7346\n",
      "Epoch: 0004/0100 | Batch 0050/0540 | Loss: 0.6220\n",
      "Epoch: 0004/0100 | Batch 0100/0540 | Loss: 0.6634\n",
      "Epoch: 0004/0100 | Batch 0150/0540 | Loss: 0.7231\n",
      "Epoch: 0004/0100 | Batch 0200/0540 | Loss: 0.4587\n",
      "Epoch: 0004/0100 | Batch 0250/0540 | Loss: 0.6083\n",
      "Epoch: 0004/0100 | Batch 0300/0540 | Loss: 0.5420\n",
      "Epoch: 0004/0100 | Batch 0350/0540 | Loss: 0.5853\n",
      "Epoch: 0004/0100 | Batch 0400/0540 | Loss: 0.4371\n",
      "Epoch: 0004/0100 | Batch 0450/0540 | Loss: 0.6375\n",
      "Epoch: 0004/0100 | Batch 0500/0540 | Loss: 0.5388\n",
      "Training metrics: {'accuracy': 0.7006365740740741, 'f1': 0.7283801522709372}%\n",
      "Valid metrics: {'accuracy': 0.7083333333333334, 'f1': 0.7388059701492536}%\n",
      "Time elapsed: 25.59 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005/0100 | Batch 0000/0540 | Loss: 0.5784\n",
      "Epoch: 0005/0100 | Batch 0050/0540 | Loss: 0.6426\n",
      "Epoch: 0005/0100 | Batch 0100/0540 | Loss: 0.4216\n",
      "Epoch: 0005/0100 | Batch 0150/0540 | Loss: 0.5253\n",
      "Epoch: 0005/0100 | Batch 0200/0540 | Loss: 0.5357\n",
      "Epoch: 0005/0100 | Batch 0250/0540 | Loss: 0.3689\n",
      "Epoch: 0005/0100 | Batch 0300/0540 | Loss: 0.2988\n",
      "Epoch: 0005/0100 | Batch 0350/0540 | Loss: 0.5290\n",
      "Epoch: 0005/0100 | Batch 0400/0540 | Loss: 0.3966\n",
      "Epoch: 0005/0100 | Batch 0450/0540 | Loss: 0.5680\n",
      "Epoch: 0005/0100 | Batch 0500/0540 | Loss: 0.3909\n",
      "Training metrics: {'accuracy': 0.8372106481481482, 'f1': 0.8690348712696122}%\n",
      "Valid metrics: {'accuracy': 0.7973958333333333, 'f1': 0.8374425407438362}%\n",
      "Time elapsed: 33.56 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0006/0100 | Batch 0000/0540 | Loss: 0.3559\n",
      "Epoch: 0006/0100 | Batch 0050/0540 | Loss: 0.4581\n",
      "Epoch: 0006/0100 | Batch 0100/0540 | Loss: 0.3671\n",
      "Epoch: 0006/0100 | Batch 0150/0540 | Loss: 0.4718\n",
      "Epoch: 0006/0100 | Batch 0200/0540 | Loss: 0.5333\n",
      "Epoch: 0006/0100 | Batch 0250/0540 | Loss: 0.3611\n",
      "Epoch: 0006/0100 | Batch 0300/0540 | Loss: 0.3494\n",
      "Epoch: 0006/0100 | Batch 0350/0540 | Loss: 0.2886\n",
      "Epoch: 0006/0100 | Batch 0400/0540 | Loss: 0.3979\n",
      "Epoch: 0006/0100 | Batch 0450/0540 | Loss: 0.3670\n",
      "Epoch: 0006/0100 | Batch 0500/0540 | Loss: 0.4053\n",
      "Training metrics: {'accuracy': 0.8926504629629629, 'f1': 0.9162641628673318}%\n",
      "Valid metrics: {'accuracy': 0.825, 'f1': 0.8650602409638554}%\n",
      "Time elapsed: 41.47 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0007/0100 | Batch 0000/0540 | Loss: 0.4923\n",
      "Epoch: 0007/0100 | Batch 0050/0540 | Loss: 0.4560\n",
      "Epoch: 0007/0100 | Batch 0100/0540 | Loss: 0.1444\n",
      "Epoch: 0007/0100 | Batch 0150/0540 | Loss: 0.2568\n",
      "Epoch: 0007/0100 | Batch 0200/0540 | Loss: 0.3323\n",
      "Epoch: 0007/0100 | Batch 0250/0540 | Loss: 0.1916\n",
      "Epoch: 0007/0100 | Batch 0300/0540 | Loss: 0.3878\n",
      "Epoch: 0007/0100 | Batch 0350/0540 | Loss: 0.2723\n",
      "Epoch: 0007/0100 | Batch 0400/0540 | Loss: 0.3427\n",
      "Epoch: 0007/0100 | Batch 0450/0540 | Loss: 0.4365\n",
      "Epoch: 0007/0100 | Batch 0500/0540 | Loss: 0.3216\n",
      "Training metrics: {'accuracy': 0.9177662037037037, 'f1': 0.935820423648435}%\n",
      "Valid metrics: {'accuracy': 0.8291666666666667, 'f1': 0.8692185007974481}%\n",
      "Time elapsed: 49.37 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0008/0100 | Batch 0000/0540 | Loss: 0.1603\n",
      "Epoch: 0008/0100 | Batch 0050/0540 | Loss: 0.1259\n",
      "Epoch: 0008/0100 | Batch 0100/0540 | Loss: 0.3004\n",
      "Epoch: 0008/0100 | Batch 0150/0540 | Loss: 0.1821\n",
      "Epoch: 0008/0100 | Batch 0200/0540 | Loss: 0.2238\n",
      "Epoch: 0008/0100 | Batch 0250/0540 | Loss: 0.2234\n",
      "Epoch: 0008/0100 | Batch 0300/0540 | Loss: 0.4191\n",
      "Epoch: 0008/0100 | Batch 0350/0540 | Loss: 0.2302\n",
      "Epoch: 0008/0100 | Batch 0400/0540 | Loss: 0.2550\n",
      "Epoch: 0008/0100 | Batch 0450/0540 | Loss: 0.4305\n",
      "Epoch: 0008/0100 | Batch 0500/0540 | Loss: 0.2485\n",
      "Training metrics: {'accuracy': 0.9487847222222222, 'f1': 0.95871623827961}%\n",
      "Valid metrics: {'accuracy': 0.8458333333333333, 'f1': 0.8756302521008403}%\n",
      "Time elapsed: 57.27 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0009/0100 | Batch 0000/0540 | Loss: 0.2650\n",
      "Epoch: 0009/0100 | Batch 0050/0540 | Loss: 0.1095\n",
      "Epoch: 0009/0100 | Batch 0100/0540 | Loss: 0.1863\n",
      "Epoch: 0009/0100 | Batch 0150/0540 | Loss: 0.1272\n",
      "Epoch: 0009/0100 | Batch 0200/0540 | Loss: 0.2077\n",
      "Epoch: 0009/0100 | Batch 0250/0540 | Loss: 0.2080\n",
      "Epoch: 0009/0100 | Batch 0300/0540 | Loss: 0.3144\n",
      "Epoch: 0009/0100 | Batch 0350/0540 | Loss: 0.5061\n",
      "Epoch: 0009/0100 | Batch 0400/0540 | Loss: 0.1703\n",
      "Epoch: 0009/0100 | Batch 0450/0540 | Loss: 0.1008\n",
      "Epoch: 0009/0100 | Batch 0500/0540 | Loss: 0.1985\n",
      "Training metrics: {'accuracy': 0.9491319444444445, 'f1': 0.9583708264267108}%\n",
      "Valid metrics: {'accuracy': 0.81875, 'f1': 0.8477690288713912}%\n",
      "Time elapsed: 65.16 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010/0100 | Batch 0000/0540 | Loss: 0.0369\n",
      "Epoch: 0010/0100 | Batch 0050/0540 | Loss: 0.1665\n",
      "Epoch: 0010/0100 | Batch 0100/0540 | Loss: 0.2317\n",
      "Epoch: 0010/0100 | Batch 0150/0540 | Loss: 0.1235\n",
      "Epoch: 0010/0100 | Batch 0200/0540 | Loss: 0.0907\n",
      "Epoch: 0010/0100 | Batch 0250/0540 | Loss: 0.0995\n",
      "Epoch: 0010/0100 | Batch 0300/0540 | Loss: 0.0924\n",
      "Epoch: 0010/0100 | Batch 0350/0540 | Loss: 0.0880\n",
      "Epoch: 0010/0100 | Batch 0400/0540 | Loss: 0.0983\n",
      "Epoch: 0010/0100 | Batch 0450/0540 | Loss: 0.2983\n",
      "Epoch: 0010/0100 | Batch 0500/0540 | Loss: 0.2550\n",
      "Training metrics: {'accuracy': 0.9758680555555556, 'f1': 0.9809597735263229}%\n",
      "Valid metrics: {'accuracy': 0.834375, 'f1': 0.8746056782334386}%\n",
      "Time elapsed: 73.05 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0011/0100 | Batch 0000/0540 | Loss: 0.1784\n",
      "Epoch: 0011/0100 | Batch 0050/0540 | Loss: 0.1247\n",
      "Epoch: 0011/0100 | Batch 0100/0540 | Loss: 0.0853\n",
      "Epoch: 0011/0100 | Batch 0150/0540 | Loss: 0.1776\n",
      "Epoch: 0011/0100 | Batch 0200/0540 | Loss: 0.1725\n",
      "Epoch: 0011/0100 | Batch 0250/0540 | Loss: 0.2205\n",
      "Epoch: 0011/0100 | Batch 0300/0540 | Loss: 0.0680\n",
      "Epoch: 0011/0100 | Batch 0350/0540 | Loss: 0.0268\n",
      "Epoch: 0011/0100 | Batch 0400/0540 | Loss: 0.0997\n",
      "Epoch: 0011/0100 | Batch 0450/0540 | Loss: 0.0396\n",
      "Epoch: 0011/0100 | Batch 0500/0540 | Loss: 0.1452\n",
      "Training metrics: {'accuracy': 0.9824652777777778, 'f1': 0.9860670437301696}%\n",
      "Valid metrics: {'accuracy': 0.8385416666666666, 'f1': 0.8759007205764612}%\n",
      "Time elapsed: 80.95 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0012/0100 | Batch 0000/0540 | Loss: 0.0498\n",
      "Epoch: 0012/0100 | Batch 0050/0540 | Loss: 0.0256\n",
      "Epoch: 0012/0100 | Batch 0100/0540 | Loss: 0.0495\n",
      "Epoch: 0012/0100 | Batch 0150/0540 | Loss: 0.0497\n",
      "Epoch: 0012/0100 | Batch 0200/0540 | Loss: 0.1172\n",
      "Epoch: 0012/0100 | Batch 0250/0540 | Loss: 0.0461\n",
      "Epoch: 0012/0100 | Batch 0300/0540 | Loss: 0.2662\n",
      "Epoch: 0012/0100 | Batch 0350/0540 | Loss: 0.0915\n",
      "Epoch: 0012/0100 | Batch 0400/0540 | Loss: 0.0552\n",
      "Epoch: 0012/0100 | Batch 0450/0540 | Loss: 0.0431\n",
      "Epoch: 0012/0100 | Batch 0500/0540 | Loss: 0.2111\n",
      "Training metrics: {'accuracy': 0.9813078703703704, 'f1': 0.9849578540492712}%\n",
      "Valid metrics: {'accuracy': 0.8203125, 'f1': 0.8514851485148516}%\n",
      "Time elapsed: 88.84 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0013/0100 | Batch 0000/0540 | Loss: 0.0172\n",
      "Epoch: 0013/0100 | Batch 0050/0540 | Loss: 0.0169\n",
      "Epoch: 0013/0100 | Batch 0100/0540 | Loss: 0.1370\n",
      "Epoch: 0013/0100 | Batch 0150/0540 | Loss: 0.0096\n",
      "Epoch: 0013/0100 | Batch 0200/0540 | Loss: 0.0661\n",
      "Epoch: 0013/0100 | Batch 0250/0540 | Loss: 0.0183\n",
      "Epoch: 0013/0100 | Batch 0300/0540 | Loss: 0.0323\n",
      "Epoch: 0013/0100 | Batch 0350/0540 | Loss: 0.0353\n",
      "Epoch: 0013/0100 | Batch 0400/0540 | Loss: 0.1551\n",
      "Epoch: 0013/0100 | Batch 0450/0540 | Loss: 0.0150\n",
      "Epoch: 0013/0100 | Batch 0500/0540 | Loss: 0.0171\n",
      "Training metrics: {'accuracy': 0.990625, 'f1': 0.9925318089618292}%\n",
      "Valid metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8686371100164204}%\n",
      "Time elapsed: 96.93 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0014/0100 | Batch 0000/0540 | Loss: 0.0462\n",
      "Epoch: 0014/0100 | Batch 0050/0540 | Loss: 0.1521\n",
      "Epoch: 0014/0100 | Batch 0100/0540 | Loss: 0.1544\n",
      "Epoch: 0014/0100 | Batch 0150/0540 | Loss: 0.0408\n",
      "Epoch: 0014/0100 | Batch 0200/0540 | Loss: 0.1000\n",
      "Epoch: 0014/0100 | Batch 0250/0540 | Loss: 0.0293\n",
      "Epoch: 0014/0100 | Batch 0300/0540 | Loss: 0.0048\n",
      "Epoch: 0014/0100 | Batch 0350/0540 | Loss: 0.1256\n",
      "Epoch: 0014/0100 | Batch 0400/0540 | Loss: 0.0114\n",
      "Epoch: 0014/0100 | Batch 0450/0540 | Loss: 0.0062\n",
      "Epoch: 0014/0100 | Batch 0500/0540 | Loss: 0.0364\n",
      "Training metrics: {'accuracy': 0.9939814814814815, 'f1': 0.9952082565425728}%\n",
      "Valid metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8697068403908794}%\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    # Unfreeze all layers after few epochs\n",
    "    if epoch == freeze_steps:\n",
    "        model.requires_grad_(True)\n",
    "            \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        ### Logging\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        print(f'Training metrics: '\n",
    "              f'{compute_metrics(model, train_loader, DEVICE)}%'\n",
    "              f'\\nValid metrics: '\n",
    "              f'{compute_metrics(model, valid_loader, DEVICE)}%')\n",
    "\n",
    "        current_f1 = compute_metrics(model, valid_loader, DEVICE)['f1']\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "            # Save the new best model\n",
    "            path = os.path.join(model_dir ,'bertweet_italian_mod.pt')\n",
    "            torch.save(model, path)\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_since_improvement >= 3:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 1.12.1 (Python 3.10)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
