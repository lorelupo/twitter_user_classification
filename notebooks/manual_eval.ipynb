{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare manual eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 37)\n",
      "Index(['tweet_id', 'user_id', 'created_at', 'tweet', 'status', 'regex_type',\n",
      "       'regex_idx', 'age_raw', 'year_tweet', 'age_in_2023', 'age_when_tweeted',\n",
      "       'username', 'full_name', 'location', 'join_year', 'join_month', 'bio',\n",
      "       'join_day', 'tweets', 'following', 'followers', 'likes', 'loc_count',\n",
      "       'location_clean', 'foreign_country', 'all_regions', 'region_pos',\n",
      "       'region', 'term_for_italy', 'name_city_engl', 'condition', 'city_id',\n",
      "       'all_cities', 'city_pos', 'region_code', 'is_male', 'user_has_image'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read pandas dataframe from pickle file\n",
    "df = pd.read_pickle('../data/user_classification/user_age_gender_location_manual_eval.pkl')\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns but \"full name\" and \"is_male\"\n",
    "dfp = df.drop([c for c in df.columns if (c != \"full_name\" and c != \"is_male\" and c != \"user_id\" and c != \"username\")], axis=1)\n",
    "# sort by is_male\n",
    "dfp.sort_values('is_male', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel file\n",
    "dfp.to_excel('../data/user_classification/gender_manual_eval_v2.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Gender Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pred_col = \"is_male\"\n",
    "annot1_col = \"lore\"\n",
    "annot2_col = \"mari\"\n",
    "annot3_col = \"ema\"\n",
    "\n",
    "dfgen = pd.read_excel(\"../data/user_classification/manual_eval_gender.xlsx\")\n",
    "dfgen[pred_col] = dfgen[pred_col].astype(int).astype(str)\n",
    "dfgen[annot1_col] = dfgen[annot1_col].fillna(dfgen[pred_col]).astype(str)\n",
    "dfgen[annot2_col] = dfgen[annot2_col].fillna(dfgen[pred_col]).astype(str)\n",
    "dfgen[annot3_col] = dfgen[annot3_col].fillna(dfgen[pred_col]).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw agreement score between annotators:\n",
      "Annotator 1 vs. Annotator 2:  0.9991666666666666\n",
      "Annotator 1 vs. Annotator 3:  1.0\n",
      "Annotator 2 vs. Annotator 3:  0.9991666666666666\n",
      "Average agreement score:  0.9994444444444444\n",
      "\n",
      "\n",
      "Raw agreement score between the automatic labelling and each annotator:\n",
      "Automatic vs. Annotator 1:  0.9941666666666666\n",
      "Automatic vs. Annotator 2:  0.9933333333333333\n",
      "Automatic vs. Annotator 3:  0.9941666666666666\n",
      "Average agreement score:  0.9938888888888888\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw agreement score between annotators:\")\n",
    "ag12 = sum(dfgen[annot1_col] == dfgen[annot2_col]) / len(dfgen)\n",
    "ag13 = sum(dfgen[annot1_col] == dfgen[annot3_col]) / len(dfgen)\n",
    "ag23 = sum(dfgen[annot2_col] == dfgen[annot3_col]) / len(dfgen)\n",
    "print(\"Annotator 1 vs. Annotator 2: \", ag12)\n",
    "print(\"Annotator 1 vs. Annotator 3: \", ag13)\n",
    "print(\"Annotator 2 vs. Annotator 3: \", ag23)\n",
    "\n",
    "ag_avg = (ag12 + ag13 + ag23) / 3\n",
    "print(\"Average agreement score: \", ag_avg)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Raw agreement score between the automatic labelling and each annotator:\")\n",
    "ag1p = sum(dfgen[pred_col] == dfgen[annot1_col]) / len(dfgen)\n",
    "ag2p = sum(dfgen[pred_col] == dfgen[annot2_col]) / len(dfgen)\n",
    "ag3p = sum(dfgen[pred_col] == dfgen[annot3_col]) / len(dfgen)\n",
    "print(\"Automatic vs. Annotator 1: \", ag1p)\n",
    "print(\"Automatic vs. Annotator 2: \", ag2p)\n",
    "print(\"Automatic vs. Annotator 3: \", ag3p)\n",
    "\n",
    "ag_avg = (ag1p + ag2p + ag3p) / 3\n",
    "print(\"Average agreement score: \", ag_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's k agreement score between annotators:\n",
      "annot1 and annot2: 0.9982112272322766\n",
      "annot1 and annot3: 1.0\n",
      "annot2 and annot3: 0.9982112272322766\n",
      "AVERAGE: 0.9988074848215177\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# compute Cohen's k agreement score between annotators\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print(\"Cohen's k agreement score between annotators:\")\n",
    "print(\"annot1 and annot2:\", cohen_kappa_score(dfgen[annot1_col], dfgen[annot2_col]))\n",
    "print(\"annot1 and annot3:\", cohen_kappa_score(dfgen[annot1_col], dfgen[annot3_col]))\n",
    "print(\"annot2 and annot3:\", cohen_kappa_score(dfgen[annot2_col], dfgen[annot3_col]))\n",
    "\n",
    "print(\"AVERAGE:\", (cohen_kappa_score(dfgen[annot1_col], dfgen[annot2_col]) + cohen_kappa_score(dfgen[annot1_col], dfgen[annot3_col]) + cohen_kappa_score(dfgen[annot2_col], dfgen[annot3_col])) / 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report of predictions against annotators:\n",
      "annot1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       430\n",
      "           1       1.00      0.99      1.00       770\n",
      "         unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99      1200\n",
      "   macro avg       0.67      0.66      0.66      1200\n",
      "weighted avg       1.00      0.99      1.00      1200\n",
      "\n",
      "annot2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       430\n",
      "           1       1.00      0.99      0.99       770\n",
      "         unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99      1200\n",
      "   macro avg       0.66      0.66      0.66      1200\n",
      "weighted avg       1.00      0.99      1.00      1200\n",
      "\n",
      "annot3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       430\n",
      "           1       1.00      0.99      1.00       770\n",
      "         unk       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99      1200\n",
      "   macro avg       0.67      0.66      0.66      1200\n",
      "weighted avg       1.00      0.99      1.00      1200\n",
      "\n",
      "AVERAGE:\n",
      "{'precision': 0.6651234567901234, 'recall': 0.6636363636363637, 'f1-score': 0.6643712014464963, 'support': 1200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/llupo/miniconda3/envs/mentalenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# compute precision, recall and f1-score of predictions against each annotator, and their average\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification report of predictions against annotators:\")\n",
    "print(\"annot1:\")\n",
    "print(classification_report(dfgen[pred_col], dfgen[annot1_col]))\n",
    "print(\"annot2:\")\n",
    "print(classification_report(dfgen[pred_col], dfgen[annot2_col]))\n",
    "print(\"annot3:\")\n",
    "print(classification_report(dfgen[pred_col], dfgen[annot3_col]))\n",
    "\n",
    "print(\"AVERAGE:\")\n",
    "print(classification_report(dfgen[pred_col], dfgen[annot1_col], output_dict=True)[\"macro avg\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/user_classification/user_age_gender_location.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share living in Foreign Country: 2.599%\n",
      "Share living in Foreign Country over total mentions: 4.805%\n"
     ]
    }
   ],
   "source": [
    "living_abroad = (df.foreign_country == '1').sum()\n",
    "living_in_italy = (df.foreign_country == '0').sum()\n",
    "mentioning_location = df[df.location!=''].location.shape[0]\n",
    "print(f\"Share users living in Foreign Country: {living_abroad / len(df) * 100:.3f}%\")\n",
    "print(f\"Share users living in Foreign Country over total mentions: {living_abroad / mentioning_location * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This row throws and error: Oggi compio 21 anni ! ‚ù§Ô∏èüíôüòòüòçüíõüíúüíöüòã‚úåÔ∏èüêØ‚úåÔ∏è‚úåÔ∏èüòãüíöüíúüíúüòçüòò‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏èüòçüíõüí™üëèüí•üë®\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        language = \"error\"\n",
    "        print(\"This row throws and error:\", text)\n",
    "    return language\n",
    "\n",
    "# detect language of each tweet\n",
    "df['lang'] = df['tweet'].apply(lambda x: detect_language(str(x)))\n",
    "# the tweet throws an error when trying to detect the language is italian, so we manually set it to italian\n",
    "df.loc[df.lang == 'error', 'lang'] = 'it'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of tweets in italian: 96.698%\n",
      "Share of users who posted at least a tweet in italian: 96.698%\n"
     ]
    }
   ],
   "source": [
    "# count share of tweets in italian\n",
    "print(f\"Share of tweets in italian: {df[df.lang=='it'].shape[0] / df.shape[0] * 100:.3f}%\")\n",
    "# count share of users who posted at least a tweet in italian\n",
    "print(f\"Share of users who posted at least a tweet in italian: {df[df.lang=='it'].user_id.nunique() / df.user_id.nunique() * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10927"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all value counts of location\n",
    "pd.set_option('display.max_rows', None)\n",
    "df[df.location!=''].location.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
