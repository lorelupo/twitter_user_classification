{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 15773\n",
      "Number of unique user_ids: 15764\n",
      "Number of duplicate user_ids: 2\n",
      "Number of empty user_ids: 2\n"
     ]
    }
   ],
   "source": [
    "# read csv table\n",
    "inpath = '../../../pappa/data/gender_classification/user_age_gender.csv'\n",
    "df = pd.read_csv(inpath, sep=';', index_col=0)\n",
    "# count number of rows and of unique user_ids\n",
    "print('Number of rows:', df.shape[0])\n",
    "print('Number of unique user_ids:', len(df.user_id.unique()))\n",
    "# retrieve duplicate user_ids\n",
    "duplicate_user_ids = df[df.duplicated(subset=['user_id'], keep=False)].user_id.unique()\n",
    "print('Number of duplicate user_ids:', len(duplicate_user_ids))\n",
    "duplicate_user_ids\n",
    "# retrieve empty user_ids\n",
    "empty_user_ids = df[df.user_id.isna()].index\n",
    "print('Number of empty user_ids:', len(empty_user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_users_tweets(\n",
    "        cursor:sqlite3.Cursor,\n",
    "        table:str,\n",
    "        user_id:list,\n",
    "        max_tweets:int=100,\n",
    "        tweets_features:list=['tweet_id', 'created_at', 'text', 'retweet_text',],\n",
    "        ):\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table}\n",
    "        WHERE user_id == {user_id}\n",
    "        LIMIT {max_tweets};\n",
    "        \"\"\"\n",
    "    \n",
    "    # Execute the query and fetch the results\n",
    "    cursor.execute(query)\n",
    "    tweets = cursor.fetchall()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "# Example usage\n",
    "database_path = '../mydata/database/myMENTALISM.db'\n",
    "table_name = 'sample_tweets'\n",
    "user_ids_table = '../../../pappa/data/user_classification/user_age_gender_location.pkl'\n",
    "user_ids_to_retrieve = pd.read_pickle(user_ids_table).user_id[:100].astype(int).tolist()\n",
    "#user_ids_to_retrieve = ['842452578738806784', '69150122']\n",
    "max_tweets_to_retrieve = 100000\n",
    "tweets_features_to_retrieve = ['tweet_id', 'created_at', 'text', 'retweet_text']\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "result = {}\n",
    "# loop through batches of user_ids\n",
    "for u in user_ids_to_retrieve:\n",
    "    # retrieve tweets\n",
    "    tweets = retrieve_users_tweets(cursor, table_name, u, max_tweets_to_retrieve, tweets_features_to_retrieve)\n",
    "    # save the tweets in a dictionary\n",
    "    result[u] = tweets\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets retrieved: 888\n"
     ]
    }
   ],
   "source": [
    "# count all tweets retrieved\n",
    "total_tweets = 0\n",
    "for k in result.keys():\n",
    "    total_tweets += len(result[k])\n",
    "print('Total tweets retrieved:', total_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is too slow, let's try to optimize it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 2500000/4637193 [00:29<00:25, 85010.93row/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 177 tweets to user_tweets_chunk0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 177 tweets to user_tweets_chunk0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 202 tweets to user_tweets_chunk0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 100 tweets to user_tweets_chunk0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m offset \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, total_rows, chunk_size):\n\u001b[1;32m     54\u001b[0m     \u001b[39m# Query the database for a chunk of rows\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     cursor\u001b[39m.\u001b[39mexecute(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSELECT * FROM \u001b[39m\u001b[39m{\u001b[39;00mtable_name\u001b[39m}\u001b[39;00m\u001b[39m LIMIT \u001b[39m\u001b[39m{\u001b[39;00mchunk_size\u001b[39m}\u001b[39;00m\u001b[39m OFFSET \u001b[39m\u001b[39m{\u001b[39;00moffset\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m     rows \u001b[39m=\u001b[39m cursor\u001b[39m.\u001b[39;49mfetchall()\n\u001b[1;32m     58\u001b[0m     \u001b[39m# Create a DataFrame from the fetched rows\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     chunk_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(rows, columns\u001b[39m=\u001b[39mcolumn_names)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "TABLE_COLUMN_NAMES = [\n",
    "    \"tweet_id\",\n",
    "    \"user_id\",\n",
    "    \"created_at\",\n",
    "    \"text\",\n",
    "    \"retweet_text\",\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "user_ids_table = '../data/user_classification/user_age_gender_location.pkl'\n",
    "db_file = '../../mentalism/sentemb/mydata/database/myMENTALISM.db'\n",
    "table_name = 'sample_tweets'\n",
    "user_ids_to_retrieve = pd.read_pickle(user_ids_table).user_id.astype(int).tolist()\n",
    "#user_ids_to_retrieve = [int('842452578738806784'), int('69150122')]\n",
    "tweets_features_to_retrieve = ['tweet_id', 'created_at', 'text', 'retweet_text']\n",
    "remove_columns = None\n",
    "chunk_size = 1000000\n",
    "max_tweets_per_user = 3\n",
    "max_users_per_chunk = 5000\n",
    "n_files=0\n",
    "\n",
    "\n",
    "# Create a database connection\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get the total number of rows\n",
    "cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "total_rows = cursor.fetchone()[0]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a tqdm progress bar\n",
    "progress_bar = tqdm(total=total_rows, unit=\"row\", desc=\"Processing\")\n",
    "\n",
    "column_names = TABLE_COLUMN_NAMES\n",
    "\n",
    "# Split user_ids_to_retrieve in chunks of max_user_ids_per_file\n",
    "user_ids_to_retrieve_chunks = [\n",
    "    user_ids_to_retrieve[i:i + max_users_per_chunk] for i in range(0, len(user_ids_to_retrieve), max_users_per_chunk)\n",
    "    ]\n",
    "\n",
    "# Loop through user_ids_to_retrieve in chunks\n",
    "for user_ids_to_retrieve in user_ids_to_retrieve_chunks:\n",
    "    # Loop through the data in chunks\n",
    "    for offset in range(0, total_rows, chunk_size):\n",
    "        # Query the database for a chunk of rows\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT {chunk_size} OFFSET {offset}\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Create a DataFrame from the fetched rows\n",
    "        chunk_df = pd.DataFrame(rows, columns=column_names)\n",
    "        \n",
    "        # Remove the unwanted columns\n",
    "        if remove_columns is not None:\n",
    "            chunk_df = chunk_df.drop(columns=remove_columns)\n",
    "\n",
    "        # Append to the result DataFrame\n",
    "        chunk_df = chunk_df[chunk_df['user_id'].isin(user_ids_to_retrieve)]\n",
    "        result_df = pd.concat([result_df, chunk_df], ignore_index=True)\n",
    "\n",
    "        # Sort the result DataFrame by user_id and created_at\n",
    "        result_df = result_df.sort_values(by=['user_id','created_at'], ascending=False)\n",
    "        # Keep only the first max_tweets_per_user tweets for each user\n",
    "        result_df = result_df.groupby('user_id').head(max_tweets_per_user)\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(len(rows))\n",
    "\n",
    "    # Save results to pickle file\n",
    "    print(f'Saving {len(result_df)} tweets to user_tweets_chunk{n_files}.pkl')\n",
    "    result_df.to_pickle(f'../data/user_tweets_chunk{n_files}.pkl')\n",
    "    # Reset the result DataFrame\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "# Close the tqdm progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20198"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids_to_retrieve = pd.read_pickle(user_ids_table).user_id.astype(int).tolist()\n",
    "len(user_ids_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fetch 1000 rows: 0.0034177303314208984 seconds\n",
      "Time to concat 1000 rows: 0.0020749568939208984 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "TABLE_COLUMN_NAMES = [\n",
    "    \"tweet_id\",\n",
    "    \"user_id\",\n",
    "    \"created_at\",\n",
    "    \"text\",\n",
    "    \"retweet_text\",\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "user_ids_table = '../data/user_classification/user_age_gender_location.pkl'\n",
    "db_file = '../../mentalism/sentemb/mydata/database/myMENTALISM.db'\n",
    "table_name = 'sample_tweets'\n",
    "user_ids_to_retrieve = pd.read_pickle(user_ids_table).user_id.astype(int).tolist()\n",
    "#user_ids_to_retrieve = [int('842452578738806784'), int('69150122')]\n",
    "max_tweets_to_retrieve = 100\n",
    "chunk_size = 1000\n",
    "tweets_features_to_retrieve = ['tweet_id', 'created_at', 'text', 'retweet_text']\n",
    "remove_columns = None\n",
    "max_tweets_per_user = 1000\n",
    "n_files=0\n",
    "\n",
    "# Create a database connection\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "column_names = TABLE_COLUMN_NAMES\n",
    "\n",
    "# Query the database for a chunk of rows\n",
    "before = time.time()\n",
    "cursor.execute(f\"SELECT * FROM {table_name} LIMIT {chunk_size} OFFSET {0}\")\n",
    "rows = cursor.fetchall()\n",
    "after = time.time()\n",
    "print(f\"Time to fetch {chunk_size} rows: {after-before} seconds\")\n",
    "\n",
    "# Create a DataFrame from the fetched rows\n",
    "chunk_df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "# Append to the result DataFrame\n",
    "# chunk_df['user_id'] = chunk_df['user_id'].astype(str)\n",
    "before = time.time()\n",
    "result_df = pd.concat([result_df, chunk_df[chunk_df['user_id'].isin(user_ids_to_retrieve)]], ignore_index=True)\n",
    "after = time.time()\n",
    "print(f\"Time to concat {chunk_size} rows: {after-before} seconds\")\n",
    "    \n",
    "# Remove the unwanted columns\n",
    "if remove_columns is not None:\n",
    "    chunk_df = chunk_df.drop(columns=remove_columns)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842462325839941633</td>\n",
       "      <td>842452578738806784</td>\n",
       "      <td>2017-03-16 19:47:25+00:00</td>\n",
       "      <td>Alla Andrea Pirillo https://t.co/QOwa2Kj6fw</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>364708769491464192</td>\n",
       "      <td>69150122</td>\n",
       "      <td>2013-08-06 11:25:21+00:00</td>\n",
       "      <td>#Metro, cerca di fermare banda</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>340817663829872642</td>\n",
       "      <td>69150122</td>\n",
       "      <td>2013-06-01 13:10:37+00:00</td>\n",
       "      <td>Servizio Pubblico http://t.co/K5sbkmfGGA via @...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1595686854401073153</td>\n",
       "      <td>434347177</td>\n",
       "      <td>2022-11-24 07:52:47+00:00</td>\n",
       "      <td>RT @fanpage: Vende cornetti dalla finestra di ...</td>\n",
       "      <td>Vende cornetti dalla finestra di casa. Ilaria ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1594019201781334016</td>\n",
       "      <td>434347177</td>\n",
       "      <td>2022-11-19 17:26:07+00:00</td>\n",
       "      <td>RT @ValaAfshar: This is how classically traine...</td>\n",
       "      <td>This is how classically trained musicians beau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id             user_id                 created_at  \\\n",
       "0   842462325839941633  842452578738806784  2017-03-16 19:47:25+00:00   \n",
       "1   364708769491464192            69150122  2013-08-06 11:25:21+00:00   \n",
       "2   340817663829872642            69150122  2013-06-01 13:10:37+00:00   \n",
       "4  1595686854401073153           434347177  2022-11-24 07:52:47+00:00   \n",
       "5  1594019201781334016           434347177  2022-11-19 17:26:07+00:00   \n",
       "\n",
       "                                                text  \\\n",
       "0        Alla Andrea Pirillo https://t.co/QOwa2Kj6fw   \n",
       "1                     #Metro, cerca di fermare banda   \n",
       "2  Servizio Pubblico http://t.co/K5sbkmfGGA via @...   \n",
       "4  RT @fanpage: Vende cornetti dalla finestra di ...   \n",
       "5  RT @ValaAfshar: This is how classically traine...   \n",
       "\n",
       "                                        retweet_text  \n",
       "0                                                     \n",
       "1                                               None  \n",
       "2                                                     \n",
       "4  Vende cornetti dalla finestra di casa. Ilaria ...  \n",
       "5  This is how classically trained musicians beau...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_df.groupby('user_id').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29434099.464913364"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "227486 / 4637193 * 600000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060.9866787946933"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "600000000 / 4637193 / 10 * 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2274860"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "227486 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import all files called None_{N}.pkl  \n",
    "df = pd.concat([pd.read_pickle(f'../None_{i}.pkl') for i in range(1)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ids(db_file, table_name):\n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # Query unique user IDs from the specified table\n",
    "        cursor.execute(f\"SELECT DISTINCT user_id FROM {table_name}\")\n",
    "        user_ids = [row[0] for row in cursor.fetchall()]\n",
    "        return user_ids\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error reading data from the database: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the database connection\n",
    "        conn.close()\n",
    "\n",
    "db_file = '../../mentalism/sentemb/mydata/database/myMENTALISM.db'\n",
    "table_name = 'sample_tweets'\n",
    "user_ids = get_all_user_ids(db_file, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[842452578738806784, 69150122, 434347177, 2673613019, 2988247127]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASK 2022.10 (Python 3.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
