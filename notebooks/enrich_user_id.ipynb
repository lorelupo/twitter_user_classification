{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 15773\n",
      "Number of unique user_ids: 15764\n",
      "Number of duplicate user_ids: 2\n",
      "Number of empty user_ids: 2\n"
     ]
    }
   ],
   "source": [
    "# read csv table\n",
    "inpath = '../../../pappa/data/gender_classification/user_age_gender.csv'\n",
    "df = pd.read_csv(inpath, sep=';', index_col=0)\n",
    "# count number of rows and of unique user_ids\n",
    "print('Number of rows:', df.shape[0])\n",
    "print('Number of unique user_ids:', len(df.user_id.unique()))\n",
    "# retrieve duplicate user_ids\n",
    "duplicate_user_ids = df[df.duplicated(subset=['user_id'], keep=False)].user_id.unique()\n",
    "print('Number of duplicate user_ids:', len(duplicate_user_ids))\n",
    "duplicate_user_ids\n",
    "# retrieve empty user_ids\n",
    "empty_user_ids = df[df.user_id.isna()].index\n",
    "print('Number of empty user_ids:', len(empty_user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_users_tweets(\n",
    "        cursor:sqlite3.Cursor,\n",
    "        table:str,\n",
    "        user_id:list,\n",
    "        max_tweets:int=100,\n",
    "        tweets_features:list=['tweet_id', 'created_at', 'text', 'retweet_text',],\n",
    "        ):\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table}\n",
    "        WHERE user_id == {user_id}\n",
    "        LIMIT {max_tweets};\n",
    "        \"\"\"\n",
    "    \n",
    "    # Execute the query and fetch the results\n",
    "    cursor.execute(query)\n",
    "    tweets = cursor.fetchall()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "# Example usage\n",
    "database_path = '../mydata/database/myMENTALISM.db'\n",
    "table_name = 'sample_tweets'\n",
    "user_ids_table = '../../../pappa/data/user_classification/user_age_gender_location.pkl'\n",
    "user_ids_to_retrieve = pd.read_pickle(user_ids_table).user_id[:100].astype(int).tolist()\n",
    "#user_ids_to_retrieve = ['842452578738806784', '69150122']\n",
    "max_tweets_to_retrieve = 100000\n",
    "tweets_features_to_retrieve = ['tweet_id', 'created_at', 'text', 'retweet_text']\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "result = {}\n",
    "# loop through batches of user_ids\n",
    "for u in user_ids_to_retrieve:\n",
    "    # retrieve tweets\n",
    "    tweets = retrieve_users_tweets(cursor, table_name, u, max_tweets_to_retrieve, tweets_features_to_retrieve)\n",
    "    # save the tweets in a dictionary\n",
    "    result[u] = tweets\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets retrieved: 888\n"
     ]
    }
   ],
   "source": [
    "# count all tweets retrieved\n",
    "total_tweets = 0\n",
    "for k in result.keys():\n",
    "    total_tweets += len(result[k])\n",
    "print('Total tweets retrieved:', total_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is too slow, let's try to optimize it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 37000/4637193 [00:02<03:02, 25155.34row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 0 with 1996 tweets\n",
      "Saving file 1 with 1338 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|▏         | 65000/4637193 [00:02<01:33, 49155.11row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 2 with 1315 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 91000/4637193 [00:02<01:04, 70403.55row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 3 with 1367 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▎         | 165000/4637193 [00:03<00:56, 78615.96row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 4 with 1870 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 190000/4637193 [00:04<01:03, 69576.86row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 5 with 1091 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▌         | 239000/4637193 [00:04<01:16, 57697.48row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 6 with 1916 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▋         | 293000/4637193 [00:05<01:27, 49931.58row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 7 with 1689 tweets\n",
      "Saving file 8 with 1000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 319000/4637193 [00:06<01:34, 45523.58row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 9 with 1915 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 354000/4637193 [00:07<01:43, 41421.09row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 10 with 1390 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 378000/4637193 [00:07<01:48, 39139.66row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 11 with 1810 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 414000/4637193 [00:08<01:55, 36497.07row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file 12 with 1322 tweets\n",
      "Saving file 13 with 1000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|▉         | 450000/4637193 [00:10<02:03, 34005.21row/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m# Loop through the data in chunks\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m offset \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, total_rows, chunk_size):\n\u001b[1;32m     42\u001b[0m     \u001b[39m# Query the database for a chunk of rows\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSELECT * FROM \u001b[39;49m\u001b[39m{\u001b[39;49;00mtable_name\u001b[39m}\u001b[39;49;00m\u001b[39m LIMIT \u001b[39;49m\u001b[39m{\u001b[39;49;00mchunk_size\u001b[39m}\u001b[39;49;00m\u001b[39m OFFSET \u001b[39;49m\u001b[39m{\u001b[39;49;00moffset\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m     rows \u001b[39m=\u001b[39m cursor\u001b[39m.\u001b[39mfetchall()\n\u001b[1;32m     46\u001b[0m     \u001b[39m# Create a DataFrame from the fetched rows\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|▉         | 450000/4637193 [00:27<02:03, 34005.21row/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "TABLE_COLUMN_NAMES = [\n",
    "    \"tweet_id\",\n",
    "    \"user_id\",\n",
    "    \"created_at\",\n",
    "    \"text\",\n",
    "    \"retweet_text\",\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "user_ids_table = '../../../pappa/data/user_classification/user_age_gender_location.pkl'\n",
    "db_file = '../mydata/database/myMENTALISM.db'\n",
    "table_name = 'sample_tweets'\n",
    "user_ids_to_retrieve = pd.read_pickle(user_ids_table).user_id.astype(int).tolist()\n",
    "#user_ids_to_retrieve = [int('842452578738806784'), int('69150122')]\n",
    "max_tweets_to_retrieve = 100\n",
    "chunk_size = 1000\n",
    "tweets_features_to_retrieve = ['tweet_id', 'created_at', 'text', 'retweet_text']\n",
    "remove_columns = None\n",
    "max_tweets_in_file = 1000\n",
    "n_files=0\n",
    "\n",
    "# Create a database connection\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get the total number of rows\n",
    "cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "total_rows = cursor.fetchone()[0]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a tqdm progress bar\n",
    "progress_bar = tqdm(total=total_rows, unit=\"row\", desc=\"Processing\")\n",
    "\n",
    "column_names = TABLE_COLUMN_NAMES\n",
    "\n",
    "# Loop through the data in chunks\n",
    "for offset in range(0, total_rows, chunk_size):\n",
    "    # Query the database for a chunk of rows\n",
    "    cursor.execute(f\"SELECT * FROM {table_name} LIMIT {chunk_size} OFFSET {offset}\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Create a DataFrame from the fetched rows\n",
    "    chunk_df = pd.DataFrame(rows, columns=column_names)\n",
    "    \n",
    "    # Remove the unwanted columns\n",
    "    if remove_columns is not None:\n",
    "        chunk_df = chunk_df.drop(columns=remove_columns)\n",
    "\n",
    "    # Append to the result DataFrame\n",
    "    # chunk_df['user_id'] = chunk_df['user_id'].astype(str)\n",
    "    result_df = pd.concat([result_df, chunk_df[chunk_df['user_id'].isin(user_ids_to_retrieve)]], ignore_index=True)\n",
    "\n",
    "    if len(result_df) >= max_tweets_in_file:\n",
    "        # Save results to json\n",
    "        print(f'Saving {len(result_df)} tweets to file {n_files}')\n",
    "        result_df.to_pickle(f'bo{n_files}.pkl')\n",
    "        # Reset the result DataFrame\n",
    "        result_df = pd.DataFrame()\n",
    "        n_files += 1\n",
    "\n",
    "    # Update the progress bar\n",
    "    progress_bar.update(len(rows))\n",
    "\n",
    "# Close the tqdm progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bo0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_pickle(\u001b[39m'\u001b[39;49m\u001b[39mbo0.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m df\n",
      "File \u001b[0;32m~/miniconda3/envs/mentalenv/lib/python3.10/site-packages/pandas/io/pickle.py:190\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m excs_to_catch \u001b[39m=\u001b[39m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mModuleNotFoundError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m)\n\u001b[0;32m--> 190\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    191\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    192\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    193\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    194\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    195\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    196\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m     \u001b[39m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[39m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[39m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mentalenv/lib/python3.10/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bo0.pkl'"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('bo0.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29434099.464913364"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "227486 / 4637193 * 600000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060.9866787946933"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "600000000 / 4637193 / 10 * 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2274860"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "227486 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASK 2022.10 (Python 3.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
