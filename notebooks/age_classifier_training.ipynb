{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab0aa90-74e5-4810-ad86-d62b738839e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adc8c712-0765-4adc-b32c-cae0f7c0eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline, set_seed\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2047646a-e7ce-44d7-bd59-92cae2e040f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "# SEED = 1\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.manual_seed(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8436723-322d-4fdf-99ea-143b5438eaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53404b9-bca8-4635-ad7e-579a65744b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f16b52b-23dd-448a-aaeb-3cc95a4ca6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/g100/home/userexternal/mhabibi0/'\n",
    "work_dir = '/g100_work/IscrC_mental'\n",
    "\n",
    "hdata_dir = os.path.join(home_dir, 'Data')\n",
    "wdata_dir = os.path.join(work_dir, 'data')\n",
    "uc_dir = os.path.join(wdata_dir, 'user_classification')\n",
    "model_dir = os.path.join(home_dir, 'Models', 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "814b16bf-297c-49b6-92e9-8c03e87880a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user age data\n",
    "path  = os.path.join(uc_dir, 'data_for_models_train.pkl')\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63c6498-d992-45d5-9484-42a018b70f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user age data\n",
    "path  = os.path.join(uc_dir, 'data_for_models_train.pkl')\n",
    "df = pd.read_pickle(path)\n",
    "\n",
    "\n",
    "# Discretize the 'age' column into four classes\n",
    "age_intervals = [0, 19, 30, 40, 100]\n",
    "age_labels = [0, 1, 2, 3]\n",
    "df['age_class'] = pd.cut(df['age'], bins=age_intervals, labels=age_labels, right=False)\n",
    "\n",
    "# create input text\n",
    "df['text']  = 'bio: ' + df['masked_bio'] + '. ' + 'tweets: ' + df['long_text'] \n",
    "df['text'] = df['text'].str.replace('\\r|\\n', ' ', regex=True)\n",
    "\n",
    "# train valid split\n",
    "df_train, df_valid = train_test_split(df[['user_id', 'text', 'age_class']], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b903ba3-9c33-4489-9b4d-a5aaaf587dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['text'].values\n",
    "y_train = df_train['age_class'].values\n",
    "\n",
    "X_valid = df_valid['text'].values\n",
    "y_valid = df_valid['age_class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac98f8c-9155-43f2-983c-0014b57807c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "\n",
    "            all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': macro_f1\n",
    "        }\n",
    "\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d228767-2023-4a68-9d93-09a1dc4ec57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a46ec20-c9b9-43fd-9a6b-3ad286e59e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_tokenize(X_text, tokenizer, max_length=512, batch_size=64):\n",
    "\n",
    "    # Dictionary to hold tokenized batches\n",
    "    encodings = {}\n",
    "\n",
    "    # Calculate the number of batches needed\n",
    "    num_batches = len(X_text) // batch_size + int(len(X_text) % batch_size > 0)\n",
    "\n",
    "    # Iterate over the data in batches\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min(len(X_text), (i + 1) * batch_size)\n",
    "\n",
    "        # Tokenize the current batch of texts\n",
    "        batch_encodings = tokenizer.batch_encode_plus(\n",
    "            list(X_text[batch_start:batch_end]),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        # Merge the batch tokenizations into the main dictionary\n",
    "        for key, val in batch_encodings.items():\n",
    "            if key not in encodings:\n",
    "                encodings[key] = []\n",
    "            encodings[key].extend(val)\n",
    "\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05948e2-ea65-4fbe-b3b4-19aef45f407d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Twitter XLM Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1bbfb8a-fa5b-4cae-a7b0-39df5a297ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base\")\n",
    "\n",
    "train_encodings = batch_tokenize(X_train, tokenizer)\n",
    "valid_encodings = batch_tokenize(X_valid, tokenizer)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, y_train)\n",
    "valid_dataset = TweetDataset(valid_encodings, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e42c8aa-1c0f-4d03-9241-6a448b488b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9d6cd-5fc0-4b22-81d6-461237a15e44",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### XLm Roberta + freezing + early stopping + linear schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80a9e363-ca17-4710-86b7-cb603b3f6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "num_labels = 4\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(DEVICE)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "LR = 2e-5\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Freeze all layers except the classifier for the first few epochs\n",
    "freeze_steps = 2\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name: # classifier layer\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# Initialize best accuracy and epochs since improvement\n",
    "best_f1 = 0.0\n",
    "epochs_since_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fa8cfd4-fbf6-4197-9c6a-13da4398c55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0100 | Batch 0000/0540 | Loss: 1.3829\n",
      "Epoch: 0001/0100 | Batch 0050/0540 | Loss: 1.3620\n",
      "Epoch: 0001/0100 | Batch 0100/0540 | Loss: 1.1555\n",
      "Epoch: 0001/0100 | Batch 0150/0540 | Loss: 1.3557\n",
      "Epoch: 0001/0100 | Batch 0200/0540 | Loss: 1.2825\n",
      "Epoch: 0001/0100 | Batch 0250/0540 | Loss: 1.3009\n",
      "Epoch: 0001/0100 | Batch 0300/0540 | Loss: 1.3325\n",
      "Epoch: 0001/0100 | Batch 0350/0540 | Loss: 1.1226\n",
      "Epoch: 0001/0100 | Batch 0400/0540 | Loss: 1.2715\n",
      "Epoch: 0001/0100 | Batch 0450/0540 | Loss: 1.3482\n",
      "Epoch: 0001/0100 | Batch 0500/0540 | Loss: 1.2894\n",
      "Training metrics: {'accuracy': 0.4927662037037037, 'f1': 0.16596942822605834}%\n",
      "Valid metrics: {'accuracy': 0.4796875, 'f1': 0.1631158399313904}%\n",
      "Time elapsed: 6.64 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002/0100 | Batch 0000/0540 | Loss: 1.0419\n",
      "Epoch: 0002/0100 | Batch 0050/0540 | Loss: 1.0933\n",
      "Epoch: 0002/0100 | Batch 0100/0540 | Loss: 1.1582\n",
      "Epoch: 0002/0100 | Batch 0150/0540 | Loss: 1.0571\n",
      "Epoch: 0002/0100 | Batch 0200/0540 | Loss: 1.3001\n",
      "Epoch: 0002/0100 | Batch 0250/0540 | Loss: 1.1779\n",
      "Epoch: 0002/0100 | Batch 0300/0540 | Loss: 1.2524\n",
      "Epoch: 0002/0100 | Batch 0350/0540 | Loss: 1.1405\n",
      "Epoch: 0002/0100 | Batch 0400/0540 | Loss: 1.1088\n",
      "Epoch: 0002/0100 | Batch 0450/0540 | Loss: 1.1620\n",
      "Epoch: 0002/0100 | Batch 0500/0540 | Loss: 1.0243\n",
      "Training metrics: {'accuracy': 0.49693287037037037, 'f1': 0.18529401540661372}%\n",
      "Valid metrics: {'accuracy': 0.48333333333333334, 'f1': 0.18066017400122505}%\n",
      "Time elapsed: 12.99 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0003/0100 | Batch 0000/0540 | Loss: 1.1573\n",
      "Epoch: 0003/0100 | Batch 0050/0540 | Loss: 0.9898\n",
      "Epoch: 0003/0100 | Batch 0100/0540 | Loss: 0.9920\n",
      "Epoch: 0003/0100 | Batch 0150/0540 | Loss: 1.1018\n",
      "Epoch: 0003/0100 | Batch 0200/0540 | Loss: 0.9470\n",
      "Epoch: 0003/0100 | Batch 0250/0540 | Loss: 0.9528\n",
      "Epoch: 0003/0100 | Batch 0300/0540 | Loss: 1.5733\n",
      "Epoch: 0003/0100 | Batch 0350/0540 | Loss: 0.8567\n",
      "Epoch: 0003/0100 | Batch 0400/0540 | Loss: 0.9177\n",
      "Epoch: 0003/0100 | Batch 0450/0540 | Loss: 1.1659\n",
      "Epoch: 0003/0100 | Batch 0500/0540 | Loss: 0.9696\n",
      "Training metrics: {'accuracy': 0.6236111111111111, 'f1': 0.4334317300468655}%\n",
      "Valid metrics: {'accuracy': 0.61875, 'f1': 0.4342393293117124}%\n",
      "Time elapsed: 22.23 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0004/0100 | Batch 0000/0540 | Loss: 1.1180\n",
      "Epoch: 0004/0100 | Batch 0050/0540 | Loss: 1.1963\n",
      "Epoch: 0004/0100 | Batch 0100/0540 | Loss: 0.8322\n",
      "Epoch: 0004/0100 | Batch 0150/0540 | Loss: 1.2137\n",
      "Epoch: 0004/0100 | Batch 0200/0540 | Loss: 1.1202\n",
      "Epoch: 0004/0100 | Batch 0250/0540 | Loss: 0.9887\n",
      "Epoch: 0004/0100 | Batch 0300/0540 | Loss: 0.8698\n",
      "Epoch: 0004/0100 | Batch 0350/0540 | Loss: 1.3320\n",
      "Epoch: 0004/0100 | Batch 0400/0540 | Loss: 0.9983\n",
      "Epoch: 0004/0100 | Batch 0450/0540 | Loss: 0.7749\n",
      "Epoch: 0004/0100 | Batch 0500/0540 | Loss: 1.1106\n",
      "Training metrics: {'accuracy': 0.6421296296296296, 'f1': 0.47908522410674415}%\n",
      "Valid metrics: {'accuracy': 0.6083333333333333, 'f1': 0.45220331212912634}%\n",
      "Time elapsed: 31.47 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005/0100 | Batch 0000/0540 | Loss: 0.8789\n",
      "Epoch: 0005/0100 | Batch 0050/0540 | Loss: 0.9823\n",
      "Epoch: 0005/0100 | Batch 0100/0540 | Loss: 0.9081\n",
      "Epoch: 0005/0100 | Batch 0150/0540 | Loss: 0.7682\n",
      "Epoch: 0005/0100 | Batch 0200/0540 | Loss: 0.6690\n",
      "Epoch: 0005/0100 | Batch 0250/0540 | Loss: 0.8114\n",
      "Epoch: 0005/0100 | Batch 0300/0540 | Loss: 0.8254\n",
      "Epoch: 0005/0100 | Batch 0350/0540 | Loss: 1.1351\n",
      "Epoch: 0005/0100 | Batch 0400/0540 | Loss: 0.9305\n",
      "Epoch: 0005/0100 | Batch 0450/0540 | Loss: 0.8068\n",
      "Epoch: 0005/0100 | Batch 0500/0540 | Loss: 0.8358\n",
      "Training metrics: {'accuracy': 0.692650462962963, 'f1': 0.5952034335671245}%\n",
      "Valid metrics: {'accuracy': 0.6213541666666667, 'f1': 0.5090097437781405}%\n",
      "Time elapsed: 40.71 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0006/0100 | Batch 0000/0540 | Loss: 0.9637\n",
      "Epoch: 0006/0100 | Batch 0050/0540 | Loss: 0.7158\n",
      "Epoch: 0006/0100 | Batch 0100/0540 | Loss: 0.8872\n",
      "Epoch: 0006/0100 | Batch 0150/0540 | Loss: 0.9394\n",
      "Epoch: 0006/0100 | Batch 0200/0540 | Loss: 1.1758\n",
      "Epoch: 0006/0100 | Batch 0250/0540 | Loss: 0.6760\n",
      "Epoch: 0006/0100 | Batch 0300/0540 | Loss: 0.8018\n",
      "Epoch: 0006/0100 | Batch 0350/0540 | Loss: 0.8894\n",
      "Epoch: 0006/0100 | Batch 0400/0540 | Loss: 0.7711\n",
      "Epoch: 0006/0100 | Batch 0450/0540 | Loss: 0.6981\n",
      "Epoch: 0006/0100 | Batch 0500/0540 | Loss: 0.6864\n",
      "Training metrics: {'accuracy': 0.7444444444444445, 'f1': 0.6253151997321698}%\n",
      "Valid metrics: {'accuracy': 0.6421875, 'f1': 0.5103898457479631}%\n",
      "Time elapsed: 49.95 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0007/0100 | Batch 0000/0540 | Loss: 0.7955\n",
      "Epoch: 0007/0100 | Batch 0050/0540 | Loss: 0.7709\n",
      "Epoch: 0007/0100 | Batch 0100/0540 | Loss: 0.5515\n",
      "Epoch: 0007/0100 | Batch 0150/0540 | Loss: 0.7021\n",
      "Epoch: 0007/0100 | Batch 0200/0540 | Loss: 0.6487\n",
      "Epoch: 0007/0100 | Batch 0250/0540 | Loss: 0.6602\n",
      "Epoch: 0007/0100 | Batch 0300/0540 | Loss: 0.6832\n",
      "Epoch: 0007/0100 | Batch 0350/0540 | Loss: 0.6995\n",
      "Epoch: 0007/0100 | Batch 0400/0540 | Loss: 0.5912\n",
      "Epoch: 0007/0100 | Batch 0450/0540 | Loss: 0.5852\n",
      "Epoch: 0007/0100 | Batch 0500/0540 | Loss: 0.8041\n",
      "Training metrics: {'accuracy': 0.7733796296296296, 'f1': 0.6756024303078131}%\n",
      "Valid metrics: {'accuracy': 0.6354166666666666, 'f1': 0.5030266903295681}%\n",
      "Time elapsed: 59.16 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0008/0100 | Batch 0000/0540 | Loss: 0.6836\n",
      "Epoch: 0008/0100 | Batch 0050/0540 | Loss: 0.5062\n",
      "Epoch: 0008/0100 | Batch 0100/0540 | Loss: 0.5453\n",
      "Epoch: 0008/0100 | Batch 0150/0540 | Loss: 0.7729\n",
      "Epoch: 0008/0100 | Batch 0200/0540 | Loss: 0.9313\n",
      "Epoch: 0008/0100 | Batch 0250/0540 | Loss: 0.4673\n",
      "Epoch: 0008/0100 | Batch 0300/0540 | Loss: 0.6362\n",
      "Epoch: 0008/0100 | Batch 0350/0540 | Loss: 0.4056\n",
      "Epoch: 0008/0100 | Batch 0400/0540 | Loss: 0.6429\n",
      "Epoch: 0008/0100 | Batch 0450/0540 | Loss: 0.9291\n",
      "Epoch: 0008/0100 | Batch 0500/0540 | Loss: 0.8379\n",
      "Training metrics: {'accuracy': 0.8168402777777778, 'f1': 0.7506255269804499}%\n",
      "Valid metrics: {'accuracy': 0.61875, 'f1': 0.5260190040909731}%\n",
      "Time elapsed: 68.40 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0009/0100 | Batch 0000/0540 | Loss: 0.5439\n",
      "Epoch: 0009/0100 | Batch 0050/0540 | Loss: 0.5876\n",
      "Epoch: 0009/0100 | Batch 0100/0540 | Loss: 0.4113\n",
      "Epoch: 0009/0100 | Batch 0150/0540 | Loss: 0.9630\n",
      "Epoch: 0009/0100 | Batch 0200/0540 | Loss: 0.4217\n",
      "Epoch: 0009/0100 | Batch 0250/0540 | Loss: 0.4495\n",
      "Epoch: 0009/0100 | Batch 0300/0540 | Loss: 0.6637\n",
      "Epoch: 0009/0100 | Batch 0350/0540 | Loss: 0.3469\n",
      "Epoch: 0009/0100 | Batch 0400/0540 | Loss: 0.7618\n",
      "Epoch: 0009/0100 | Batch 0450/0540 | Loss: 0.5045\n",
      "Epoch: 0009/0100 | Batch 0500/0540 | Loss: 0.6551\n",
      "Training metrics: {'accuracy': 0.8526041666666667, 'f1': 0.796701066213249}%\n",
      "Valid metrics: {'accuracy': 0.5989583333333334, 'f1': 0.5161281453485982}%\n",
      "Time elapsed: 77.61 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010/0100 | Batch 0000/0540 | Loss: 0.4720\n",
      "Epoch: 0010/0100 | Batch 0050/0540 | Loss: 0.5126\n",
      "Epoch: 0010/0100 | Batch 0100/0540 | Loss: 0.5282\n",
      "Epoch: 0010/0100 | Batch 0150/0540 | Loss: 0.2724\n",
      "Epoch: 0010/0100 | Batch 0200/0540 | Loss: 0.3552\n",
      "Epoch: 0010/0100 | Batch 0250/0540 | Loss: 0.2772\n",
      "Epoch: 0010/0100 | Batch 0300/0540 | Loss: 0.4141\n",
      "Epoch: 0010/0100 | Batch 0350/0540 | Loss: 0.3837\n",
      "Epoch: 0010/0100 | Batch 0400/0540 | Loss: 0.5272\n",
      "Epoch: 0010/0100 | Batch 0450/0540 | Loss: 0.7653\n",
      "Epoch: 0010/0100 | Batch 0500/0540 | Loss: 0.5062\n",
      "Training metrics: {'accuracy': 0.907349537037037, 'f1': 0.8730322845044426}%\n",
      "Valid metrics: {'accuracy': 0.6114583333333333, 'f1': 0.5252348915746545}%\n",
      "Time elapsed: 87.01 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0011/0100 | Batch 0000/0540 | Loss: 0.3879\n",
      "Epoch: 0011/0100 | Batch 0050/0540 | Loss: 0.7963\n",
      "Epoch: 0011/0100 | Batch 0100/0540 | Loss: 0.3133\n",
      "Epoch: 0011/0100 | Batch 0150/0540 | Loss: 0.3248\n",
      "Epoch: 0011/0100 | Batch 0200/0540 | Loss: 0.3674\n",
      "Epoch: 0011/0100 | Batch 0250/0540 | Loss: 0.4980\n",
      "Epoch: 0011/0100 | Batch 0300/0540 | Loss: 0.3030\n",
      "Epoch: 0011/0100 | Batch 0350/0540 | Loss: 0.2211\n",
      "Epoch: 0011/0100 | Batch 0400/0540 | Loss: 0.2472\n",
      "Epoch: 0011/0100 | Batch 0450/0540 | Loss: 0.3385\n",
      "Epoch: 0011/0100 | Batch 0500/0540 | Loss: 0.3872\n",
      "Training metrics: {'accuracy': 0.9189814814814815, 'f1': 0.8917001868070116}%\n",
      "Valid metrics: {'accuracy': 0.5817708333333333, 'f1': 0.5118746096345851}%\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    # Unfreeze all layers after few epochs\n",
    "    if epoch == freeze_steps:\n",
    "        model.requires_grad_(True)\n",
    "            \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        ### Logging\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        print(f'Training metrics: '\n",
    "              f'{compute_metrics(model, train_loader, DEVICE)}%'\n",
    "              f'\\nValid metrics: '\n",
    "              f'{compute_metrics(model, valid_loader, DEVICE)}%')\n",
    "\n",
    "        current_f1 = compute_metrics(model, valid_loader, DEVICE)['f1']\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "            # Save the new best model\n",
    "            path = os.path.join(model_dir ,'XLM_age.pt')\n",
    "            torch.save(model, path)\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_since_improvement >= 3:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "406e4c93-092a-4071-b32a-9cd06fe08bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/g100_work/IscrC_mental/data/user_classification/trained_models/age/XLM_age.pt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save models in work directory\n",
    "path = os.path.join(uc_dir, 'trained_models')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "sub_path = os.path.join(path, 'age')\n",
    "if not os.path.exists(sub_path):\n",
    "        os.mkdir(sub_path)\n",
    "\n",
    "# XLM\n",
    "import shutil\n",
    "source_path =  os.path.join(model_dir ,'XLM_age.pt')\n",
    "dest_path = os.path.join(sub_path, 'XLM_age.pt')\n",
    "shutil.copy(source_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d92dc8-ba38-4a6c-818f-b65bcb3e00bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##   bert-tweet-base-italian-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4be1e6d-3e6a-4d37-8fc8-48932b18140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"osiria/bert-tweet-base-italian-uncased\")\n",
    "\n",
    "train_encodings = batch_tokenize(X_train, tokenizer)\n",
    "valid_encodings = batch_tokenize(X_valid, tokenizer)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, y_train)\n",
    "valid_dataset = TweetDataset(valid_encodings, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "410c6eeb-9aa9-4f31-b0b0-db955d8f1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed014920-84f9-4f7d-8043-23ea492ab677",
   "metadata": {},
   "source": [
    "#### Bertweet + freeze + early stopping + linear schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bfa646f-f54c-417a-86e4-e1155939f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at osiria/bert-tweet-base-italian-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at osiria/bert-tweet-base-italian-uncased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "MODEL = \"osiria/bert-tweet-base-italian-uncased\"\n",
    "num_labels = 4\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(DEVICE)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "LR = 2e-5\n",
    "\n",
    "# Adam optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Freeze all layers except the classifier for the first few epochs\n",
    "freeze_steps = 2\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name: # classifier layer\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# Initialize best accuracy and epochs since improvement\n",
    "best_f1 = 0.0\n",
    "epochs_since_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f79925c7-9d4f-419a-a71a-938ebc447320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0100 | Batch 0000/0540 | Loss: 1.4175\n",
      "Epoch: 0001/0100 | Batch 0050/0540 | Loss: 1.3475\n",
      "Epoch: 0001/0100 | Batch 0100/0540 | Loss: 1.3008\n",
      "Epoch: 0001/0100 | Batch 0150/0540 | Loss: 1.2773\n",
      "Epoch: 0001/0100 | Batch 0200/0540 | Loss: 1.1173\n",
      "Epoch: 0001/0100 | Batch 0250/0540 | Loss: 1.2485\n",
      "Epoch: 0001/0100 | Batch 0300/0540 | Loss: 1.3703\n",
      "Epoch: 0001/0100 | Batch 0350/0540 | Loss: 1.2290\n",
      "Epoch: 0001/0100 | Batch 0400/0540 | Loss: 1.1605\n",
      "Epoch: 0001/0100 | Batch 0450/0540 | Loss: 1.4217\n",
      "Epoch: 0001/0100 | Batch 0500/0540 | Loss: 1.1619\n",
      "Training metrics: {'accuracy': 0.4925347222222222, 'f1': 0.16499941840176804}%\n",
      "Valid metrics: {'accuracy': 0.4791666666666667, 'f1': 0.1619718309859155}%\n",
      "Time elapsed: 5.03 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002/0100 | Batch 0000/0540 | Loss: 1.1757\n",
      "Epoch: 0002/0100 | Batch 0050/0540 | Loss: 1.0646\n",
      "Epoch: 0002/0100 | Batch 0100/0540 | Loss: 1.1214\n",
      "Epoch: 0002/0100 | Batch 0150/0540 | Loss: 1.4042\n",
      "Epoch: 0002/0100 | Batch 0200/0540 | Loss: 1.2427\n",
      "Epoch: 0002/0100 | Batch 0250/0540 | Loss: 1.2560\n",
      "Epoch: 0002/0100 | Batch 0300/0540 | Loss: 1.4770\n",
      "Epoch: 0002/0100 | Batch 0350/0540 | Loss: 1.3571\n",
      "Epoch: 0002/0100 | Batch 0400/0540 | Loss: 1.1915\n",
      "Epoch: 0002/0100 | Batch 0450/0540 | Loss: 1.1563\n",
      "Epoch: 0002/0100 | Batch 0500/0540 | Loss: 1.2304\n",
      "Training metrics: {'accuracy': 0.4925347222222222, 'f1': 0.1650058162078325}%\n",
      "Valid metrics: {'accuracy': 0.4791666666666667, 'f1': 0.1619718309859155}%\n",
      "Time elapsed: 9.92 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0003/0100 | Batch 0000/0540 | Loss: 1.0302\n",
      "Epoch: 0003/0100 | Batch 0050/0540 | Loss: 1.0997\n",
      "Epoch: 0003/0100 | Batch 0100/0540 | Loss: 1.0758\n",
      "Epoch: 0003/0100 | Batch 0150/0540 | Loss: 1.0697\n",
      "Epoch: 0003/0100 | Batch 0200/0540 | Loss: 1.0629\n",
      "Epoch: 0003/0100 | Batch 0250/0540 | Loss: 0.8222\n",
      "Epoch: 0003/0100 | Batch 0300/0540 | Loss: 1.1140\n",
      "Epoch: 0003/0100 | Batch 0350/0540 | Loss: 1.0684\n",
      "Epoch: 0003/0100 | Batch 0400/0540 | Loss: 0.9221\n",
      "Epoch: 0003/0100 | Batch 0450/0540 | Loss: 1.0059\n",
      "Epoch: 0003/0100 | Batch 0500/0540 | Loss: 1.0371\n",
      "Training metrics: {'accuracy': 0.6186921296296296, 'f1': 0.4319826400562383}%\n",
      "Valid metrics: {'accuracy': 0.5953125, 'f1': 0.4125443041992446}%\n",
      "Time elapsed: 17.87 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0004/0100 | Batch 0000/0540 | Loss: 0.7283\n",
      "Epoch: 0004/0100 | Batch 0050/0540 | Loss: 0.9104\n",
      "Epoch: 0004/0100 | Batch 0100/0540 | Loss: 1.0668\n",
      "Epoch: 0004/0100 | Batch 0150/0540 | Loss: 0.8832\n",
      "Epoch: 0004/0100 | Batch 0200/0540 | Loss: 0.9238\n",
      "Epoch: 0004/0100 | Batch 0250/0540 | Loss: 0.9687\n",
      "Epoch: 0004/0100 | Batch 0300/0540 | Loss: 1.0565\n",
      "Epoch: 0004/0100 | Batch 0350/0540 | Loss: 0.6881\n",
      "Epoch: 0004/0100 | Batch 0400/0540 | Loss: 0.9972\n",
      "Epoch: 0004/0100 | Batch 0450/0540 | Loss: 0.7962\n",
      "Epoch: 0004/0100 | Batch 0500/0540 | Loss: 1.0730\n",
      "Training metrics: {'accuracy': 0.6798032407407407, 'f1': 0.5195867016162029}%\n",
      "Valid metrics: {'accuracy': 0.640625, 'f1': 0.4740931779618318}%\n",
      "Time elapsed: 25.79 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005/0100 | Batch 0000/0540 | Loss: 0.9594\n",
      "Epoch: 0005/0100 | Batch 0050/0540 | Loss: 0.6779\n",
      "Epoch: 0005/0100 | Batch 0100/0540 | Loss: 0.8352\n",
      "Epoch: 0005/0100 | Batch 0150/0540 | Loss: 0.9399\n",
      "Epoch: 0005/0100 | Batch 0200/0540 | Loss: 0.7773\n",
      "Epoch: 0005/0100 | Batch 0250/0540 | Loss: 0.6127\n",
      "Epoch: 0005/0100 | Batch 0300/0540 | Loss: 1.0567\n",
      "Epoch: 0005/0100 | Batch 0350/0540 | Loss: 0.7110\n",
      "Epoch: 0005/0100 | Batch 0400/0540 | Loss: 0.7260\n",
      "Epoch: 0005/0100 | Batch 0450/0540 | Loss: 0.7137\n",
      "Epoch: 0005/0100 | Batch 0500/0540 | Loss: 0.9893\n",
      "Training metrics: {'accuracy': 0.7183449074074074, 'f1': 0.5681179676382347}%\n",
      "Valid metrics: {'accuracy': 0.6473958333333333, 'f1': 0.48953533400125804}%\n",
      "Time elapsed: 33.71 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0006/0100 | Batch 0000/0540 | Loss: 0.9362\n",
      "Epoch: 0006/0100 | Batch 0050/0540 | Loss: 0.4671\n",
      "Epoch: 0006/0100 | Batch 0100/0540 | Loss: 0.8211\n",
      "Epoch: 0006/0100 | Batch 0150/0540 | Loss: 0.6166\n",
      "Epoch: 0006/0100 | Batch 0200/0540 | Loss: 0.6039\n",
      "Epoch: 0006/0100 | Batch 0250/0540 | Loss: 0.9591\n",
      "Epoch: 0006/0100 | Batch 0300/0540 | Loss: 0.8554\n",
      "Epoch: 0006/0100 | Batch 0350/0540 | Loss: 0.8246\n",
      "Epoch: 0006/0100 | Batch 0400/0540 | Loss: 0.6421\n",
      "Epoch: 0006/0100 | Batch 0450/0540 | Loss: 0.9264\n",
      "Epoch: 0006/0100 | Batch 0500/0540 | Loss: 0.8052\n",
      "Training metrics: {'accuracy': 0.7668402777777777, 'f1': 0.6636790528764708}%\n",
      "Valid metrics: {'accuracy': 0.6354166666666666, 'f1': 0.5024553474457967}%\n",
      "Time elapsed: 41.63 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0007/0100 | Batch 0000/0540 | Loss: 0.9467\n",
      "Epoch: 0007/0100 | Batch 0050/0540 | Loss: 0.7749\n",
      "Epoch: 0007/0100 | Batch 0100/0540 | Loss: 0.7063\n",
      "Epoch: 0007/0100 | Batch 0150/0540 | Loss: 0.5497\n",
      "Epoch: 0007/0100 | Batch 0200/0540 | Loss: 0.9245\n",
      "Epoch: 0007/0100 | Batch 0250/0540 | Loss: 0.7558\n",
      "Epoch: 0007/0100 | Batch 0300/0540 | Loss: 0.5222\n",
      "Epoch: 0007/0100 | Batch 0350/0540 | Loss: 0.8637\n",
      "Epoch: 0007/0100 | Batch 0400/0540 | Loss: 0.6743\n",
      "Epoch: 0007/0100 | Batch 0450/0540 | Loss: 0.8855\n",
      "Epoch: 0007/0100 | Batch 0500/0540 | Loss: 0.6149\n",
      "Training metrics: {'accuracy': 0.8184606481481481, 'f1': 0.746813287203581}%\n",
      "Valid metrics: {'accuracy': 0.6322916666666667, 'f1': 0.5364795008912656}%\n",
      "Time elapsed: 49.55 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0008/0100 | Batch 0000/0540 | Loss: 1.1604\n",
      "Epoch: 0008/0100 | Batch 0050/0540 | Loss: 0.5597\n",
      "Epoch: 0008/0100 | Batch 0100/0540 | Loss: 0.4237\n",
      "Epoch: 0008/0100 | Batch 0150/0540 | Loss: 0.6647\n",
      "Epoch: 0008/0100 | Batch 0200/0540 | Loss: 0.6939\n",
      "Epoch: 0008/0100 | Batch 0250/0540 | Loss: 0.4937\n",
      "Epoch: 0008/0100 | Batch 0300/0540 | Loss: 0.7225\n",
      "Epoch: 0008/0100 | Batch 0350/0540 | Loss: 0.6767\n",
      "Epoch: 0008/0100 | Batch 0400/0540 | Loss: 0.2811\n",
      "Epoch: 0008/0100 | Batch 0450/0540 | Loss: 0.5824\n",
      "Epoch: 0008/0100 | Batch 0500/0540 | Loss: 0.4015\n",
      "Training metrics: {'accuracy': 0.8623842592592592, 'f1': 0.8157309846582538}%\n",
      "Valid metrics: {'accuracy': 0.6057291666666667, 'f1': 0.5330890378702301}%\n",
      "Time elapsed: 57.46 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0009/0100 | Batch 0000/0540 | Loss: 0.5141\n",
      "Epoch: 0009/0100 | Batch 0050/0540 | Loss: 0.2400\n",
      "Epoch: 0009/0100 | Batch 0100/0540 | Loss: 0.5066\n",
      "Epoch: 0009/0100 | Batch 0150/0540 | Loss: 0.5469\n",
      "Epoch: 0009/0100 | Batch 0200/0540 | Loss: 0.3639\n",
      "Epoch: 0009/0100 | Batch 0250/0540 | Loss: 0.4560\n",
      "Epoch: 0009/0100 | Batch 0300/0540 | Loss: 0.5901\n",
      "Epoch: 0009/0100 | Batch 0350/0540 | Loss: 0.4849\n",
      "Epoch: 0009/0100 | Batch 0400/0540 | Loss: 0.5011\n",
      "Epoch: 0009/0100 | Batch 0450/0540 | Loss: 0.2755\n",
      "Epoch: 0009/0100 | Batch 0500/0540 | Loss: 0.2781\n",
      "Training metrics: {'accuracy': 0.8854166666666666, 'f1': 0.8432749079863538}%\n",
      "Valid metrics: {'accuracy': 0.6057291666666667, 'f1': 0.5251013036546821}%\n",
      "Time elapsed: 65.38 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010/0100 | Batch 0000/0540 | Loss: 0.3254\n",
      "Epoch: 0010/0100 | Batch 0050/0540 | Loss: 0.5289\n",
      "Epoch: 0010/0100 | Batch 0100/0540 | Loss: 0.3251\n",
      "Epoch: 0010/0100 | Batch 0150/0540 | Loss: 0.3055\n",
      "Epoch: 0010/0100 | Batch 0200/0540 | Loss: 0.3582\n",
      "Epoch: 0010/0100 | Batch 0250/0540 | Loss: 0.1760\n",
      "Epoch: 0010/0100 | Batch 0300/0540 | Loss: 0.2200\n",
      "Epoch: 0010/0100 | Batch 0350/0540 | Loss: 0.4280\n",
      "Epoch: 0010/0100 | Batch 0400/0540 | Loss: 0.1992\n",
      "Epoch: 0010/0100 | Batch 0450/0540 | Loss: 0.2806\n",
      "Epoch: 0010/0100 | Batch 0500/0540 | Loss: 0.2380\n",
      "Training metrics: {'accuracy': 0.9251157407407408, 'f1': 0.8956377862939804}%\n",
      "Valid metrics: {'accuracy': 0.6125, 'f1': 0.5190090215460894}%\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    # Unfreeze all layers after few epochs\n",
    "    if epoch == freeze_steps:\n",
    "        model.requires_grad_(True)\n",
    "            \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        ### Logging\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        print(f'Training metrics: '\n",
    "              f'{compute_metrics(model, train_loader, DEVICE)}%'\n",
    "              f'\\nValid metrics: '\n",
    "              f'{compute_metrics(model, valid_loader, DEVICE)}%')\n",
    "\n",
    "        current_f1 = compute_metrics(model, valid_loader, DEVICE)['f1']\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "            # Save the new best model\n",
    "            path = os.path.join(model_dir ,'bertweet_italian_age.pt')\n",
    "            torch.save(model, path)\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_since_improvement >= 3:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f84ef2-b5c6-4d5e-9086-6f66e3b02705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models in work directory\n",
    "path = os.path.join(uc_dir, 'trained_models')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "sub_path = os.path.join(path, 'age')\n",
    "if not os.path.exists(sub_path):\n",
    "        os.mkdir(sub_path)\n",
    "\n",
    "# XLM\n",
    "import shutil\n",
    "source_path =  os.path.join(model_dir ,'bertweet_italian_age.pt')\n",
    "dest_path = os.path.join(sub_path, 'bertweet_italian_age.pt')\n",
    "shutil.copy(source_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ebbe3-0fe3-4212-9d4a-b4bbd5e3f43f",
   "metadata": {},
   "source": [
    "## Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d27db07-7f60-4431-9500-861e48e5df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user age data\n",
    "path  = os.path.join(uc_dir, 'data_for_models_test.pkl')\n",
    "df_test = pd.read_pickle(path)\n",
    "\n",
    "# Discretize the 'age' column into four classes\n",
    "age_intervals = [0, 19, 30, 40, 100]\n",
    "age_labels = [0, 1, 2, 3]\n",
    "df_test['age_class'] = pd.cut(df_test['age'], bins=age_intervals, labels=age_labels, right=False)\n",
    "\n",
    "\n",
    "df_test['text']  = 'bio: ' + df_test['masked_bio'] + '. ' + 'tweets: ' + df_test['long_text'] \n",
    "df_test['text'] = df_test['text'].str.replace('\\r|\\n', ' ', regex=True)\n",
    "\n",
    "X_test = df_test['text'].values\n",
    "y_test = df_test['age_class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d577532c-dcfc-450b-b357-ad241337db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "xlm_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base\")\n",
    "\n",
    "\n",
    "# setup tokenizer BerTweet\n",
    "btwt_tokenizer = AutoTokenizer.from_pretrained(\"osiria/bert-tweet-base-italian-uncased\")\n",
    "\n",
    "# test encodings and dataset\n",
    "xlm_test_encodings = batch_tokenize(X_test, xlm_tokenizer)\n",
    "btwt_test_encodings = batch_tokenize(X_test, btwt_tokenizer)\n",
    "\n",
    "xlm_test_dataset = TweetDataset(xlm_test_encodings, y_test)\n",
    "btwt_test_dataset = TweetDataset(btwt_test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bfef855-afe1-4d41-a041-80a18ac23217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "xlm_loader = torch.utils.data.DataLoader(\n",
    "    xlm_test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "btwt_loader = torch.utils.data.DataLoader(\n",
    "    btwt_test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de670454-1e86-44b7-a0fd-729c047ae516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model_path, data_loader, device):\n",
    "    \n",
    "    # Load the saved model\n",
    "    model = torch.load(model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # store predicted probs\n",
    "    class_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            class_probs.extend(probabilities.cpu().numpy().tolist())\n",
    "\n",
    "            \n",
    "    # Compute the metrics\n",
    "    metrics = compute_metrics(model, data_loader, device)\n",
    "    \n",
    "    return metrics, np.array(class_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3a9d4f8-abb9-4324-885d-ecde10c937cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xlm =  os.path.join(model_dir ,'XLM_age.pt')\n",
    "path_btwt  = os.path.join(model_dir ,'bertweet_italian_age.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fce9a1ce-bcbc-4f94-ba73-87b0db632895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6133928571428572, 'f1': 0.5507302799158431}\n"
     ]
    }
   ],
   "source": [
    "# XLM\n",
    "metrics, xlm_probs = test_eval(path_xlm, xlm_loader, DEVICE)\n",
    "\n",
    "# save probs\n",
    "path = os.path.join(uc_dir, 'trained_models', 'age', 'XLM_probs_age.npy')\n",
    "np.save(path, xlm_probs)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49b098cb-0fc0-463a-8811-d12734298ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.60625, 'f1': 0.5330589059575411}\n"
     ]
    }
   ],
   "source": [
    "# BerTweet\n",
    "metrics, btwt_probs = test_eval(path_btwt, btwt_loader, DEVICE)\n",
    "\n",
    "path = os.path.join(uc_dir, 'trained_models', 'age', 'BerTweet_probs_age.npy')\n",
    "np.save(path, btwt_probs)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0451d9a1-7f17-4732-b0d8-447e87225f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models in work directory\n",
    "path = os.path.join(uc_dir, 'trained_models')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "sub_path = os.path.join(path, 'age')\n",
    "if not os.path.exists(sub_path):\n",
    "        os.mkdir(sub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e443c12-ef94-4f25-80e3-c4618e52c8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/g100_work/IscrC_mental/data/user_classification/trained_models/gender/XLM_gender.pt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # XLM\n",
    "# import shutil\n",
    "# source_path =  os.path.join(model_dir ,'XLM_gender.pt')\n",
    "# dest_path = os.path.join(uc_dir, 'trained_models', 'gender', 'XLM_gender.pt')\n",
    "# shutil.copy(source_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "378a1ca2-eb98-44cb-9174-c2dc4c5672aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/g100/home/userexternal/mhabibi0/Models/Age'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3f0f9fa-a3e8-4de9-8e75-749b0a9e0a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/g100_work/IscrC_mental/data/user_classification/trained_models/age/BerTweet_age.pt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # BERTweet\n",
    "# import shutil\n",
    "# source_path =  os.path.join(model_dir ,'bertweet_italian_age.pt')\n",
    "# dest_path = os.path.join(sub_path, 'BerTweet_age.pt')\n",
    "# shutil.copy(source_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "355e1654-f83d-4ffa-bb04-8fac6f9b99d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/g100_work/IscrC_mental/data/user_classification/trained_models/age/BerTweet_age.pt'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88244b2-a41a-4d14-a15b-a874b353a63a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 1.12.1 (Python 3.10)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
