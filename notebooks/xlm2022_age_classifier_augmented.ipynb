{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab0aa90-74e5-4810-ad86-d62b738839e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc8c712-0765-4adc-b32c-cae0f7c0eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline, set_seed\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2047646a-e7ce-44d7-bd59-92cae2e040f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "# SEED = 1\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.manual_seed(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8436723-322d-4fdf-99ea-143b5438eaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53404b9-bca8-4635-ad7e-579a65744b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f16b52b-23dd-448a-aaeb-3cc95a4ca6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/g100/home/userexternal/mhabibi0/'\n",
    "work_dir = '/g100_work/IscrC_mental'\n",
    "scratch = '/g100_scratch/userexternal/mhabibi0'\n",
    "\n",
    "hdata_dir = os.path.join(home_dir, 'Data')\n",
    "wdata_dir = os.path.join(work_dir, 'data')\n",
    "uc_dir = os.path.join(wdata_dir, 'user_classification')\n",
    "model_dir = os.path.join(scratch, 'Models', 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd937b84-2920-47b4-912c-9444f470698b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac98f8c-9155-43f2-983c-0014b57807c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "\n",
    "            all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': macro_f1\n",
    "        }\n",
    "\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d228767-2023-4a68-9d93-09a1dc4ec57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a46ec20-c9b9-43fd-9a6b-3ad286e59e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_tokenize(X_text, tokenizer, max_length=512, batch_size=64):\n",
    "\n",
    "    # Dictionary to hold tokenized batches\n",
    "    encodings = {}\n",
    "\n",
    "    # Calculate the number of batches needed\n",
    "    num_batches = len(X_text) // batch_size + int(len(X_text) % batch_size > 0)\n",
    "\n",
    "    # Iterate over the data in batches\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min(len(X_text), (i + 1) * batch_size)\n",
    "\n",
    "        # Tokenize the current batch of texts\n",
    "        batch_encodings = tokenizer.batch_encode_plus(\n",
    "            list(X_text[batch_start:batch_end]),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        # Merge the batch tokenizations into the main dictionary\n",
    "        for key, val in batch_encodings.items():\n",
    "            if key not in encodings:\n",
    "                encodings[key] = []\n",
    "            encodings[key].extend(val)\n",
    "\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b356a1-4a91-482b-a77a-1e6218c1137a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d6802a-bfb9-4371-a8a5-8e19c71a60c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_local/slurm_job.11524228/ipykernel_363188/2216679107.py:29: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['username_sep'] = df['username'].str.replace(r'([a-zA-Z])(\\d)', r'\\1 \\2').\\\n",
      "/scratch_local/slurm_job.11524228/ipykernel_363188/2216679107.py:30: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  str.replace(r'(\\d)([a-zA-Z])', r'\\1 \\2')\n"
     ]
    }
   ],
   "source": [
    "# user age training data \n",
    "path  = os.path.join(uc_dir, 'data_for_models_train.pkl')\n",
    "df = pd.read_pickle(path)\n",
    "\n",
    "\n",
    "# user info data\n",
    "path = os.path.join(wdata_dir, 'database', 'user_geocoded.parquet' )\n",
    "df_users = pd.read_parquet(path)\n",
    "df_users = df_users[['user_id', 'username', 'full_name', 'location',\n",
    "          'join_year', 'tweets', 'following', 'followers']]\n",
    "\n",
    "df_users = df_users[df_users['user_id'].isin(df['user_id'].values)]\n",
    "\n",
    "\n",
    "# merge \n",
    "df = df.merge(df_users, on='user_id', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Discretize the 'age' column into six classes\n",
    "age_intervals = [0, 18, 30, 40, 60, 100]\n",
    "age_labels = [0, 1, 2, 3, 4]\n",
    "df = df[df['age']<=99]\n",
    "df['age_class'] = pd.cut(df['age'], bins=age_intervals, labels=age_labels, right=False).astype(int)\n",
    "\n",
    "# create input text\n",
    "# Separating text and numbers with a space\n",
    "df['username_sep'] = df['username'].str.replace(r'([a-zA-Z])(\\d)', r'\\1 \\2').\\\n",
    "                    str.replace(r'(\\d)([a-zA-Z])', r'\\1 \\2')\n",
    "# concat info\n",
    "df['text']  = 'NAME:' + ' \"' + df['full_name'] + '\". ' +\\\n",
    "                'USERNAME:' + ' \"'+  df['username_sep'] + '\". ' + \\\n",
    "                'JOINED:' + ' \"' + df['join_year'].astype(str) + '\". ' +\\\n",
    "                'TWEETS:' + ' \"' + df['tweets'].astype(str) + '\". ' + \\\n",
    "                'FOLLOWING:' + ' \"' + df['following'].astype(str) + '\". ' +\\\n",
    "                'FOLLOWERS:' + ' \"' + df['followers'].astype(str) + '\". ' + \\\n",
    "                'BIO:' + ' \"' + df['masked_bio'] + '\". ' + \\\n",
    "                'TEXT:' + ' \"' + df['long_text'] + '\".'\n",
    "\n",
    "# train valid split\n",
    "df_train, df_valid = train_test_split(df[['user_id', 'text', 'age_class']], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b903ba3-9c33-4489-9b4d-a5aaaf587dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['text'].values\n",
    "y_train = df_train['age_class'].values\n",
    "\n",
    "X_valid = df_valid['text'].values\n",
    "y_valid = df_valid['age_class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1bbfb8a-fa5b-4cae-a7b0-39df5a297ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-large-2022\")\n",
    "\n",
    "train_encodings = batch_tokenize(X_train, tokenizer)\n",
    "valid_encodings = batch_tokenize(X_valid, tokenizer)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, y_train)\n",
    "valid_dataset = TweetDataset(valid_encodings, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e42c8aa-1c0f-4d03-9241-6a448b488b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a9e363-ca17-4710-86b7-cb603b3f6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-xlm-roberta-large-2022\"\n",
    "num_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(DEVICE)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa8cfd4-fbf6-4197-9c6a-13da4398c55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0100 | Batch 0000/1079 | Loss: 1.5291\n",
      "Epoch: 0001/0100 | Batch 0050/1079 | Loss: 1.4623\n",
      "Epoch: 0001/0100 | Batch 0100/1079 | Loss: 1.4347\n",
      "Epoch: 0001/0100 | Batch 0150/1079 | Loss: 1.4833\n",
      "Epoch: 0001/0100 | Batch 0200/1079 | Loss: 1.6650\n",
      "Epoch: 0001/0100 | Batch 0250/1079 | Loss: 1.8413\n",
      "Epoch: 0001/0100 | Batch 0300/1079 | Loss: 1.3416\n",
      "Epoch: 0001/0100 | Batch 0350/1079 | Loss: 1.4661\n",
      "Epoch: 0001/0100 | Batch 0400/1079 | Loss: 1.4353\n",
      "Epoch: 0001/0100 | Batch 0450/1079 | Loss: 1.4625\n",
      "Epoch: 0001/0100 | Batch 0500/1079 | Loss: 1.3396\n",
      "Epoch: 0001/0100 | Batch 0550/1079 | Loss: 1.3023\n",
      "Epoch: 0001/0100 | Batch 0600/1079 | Loss: 1.5569\n",
      "Epoch: 0001/0100 | Batch 0650/1079 | Loss: 1.4079\n",
      "Epoch: 0001/0100 | Batch 0700/1079 | Loss: 1.3391\n",
      "Epoch: 0001/0100 | Batch 0750/1079 | Loss: 1.6926\n",
      "Epoch: 0001/0100 | Batch 0800/1079 | Loss: 1.3321\n",
      "Epoch: 0001/0100 | Batch 0850/1079 | Loss: 1.3401\n",
      "Epoch: 0001/0100 | Batch 0900/1079 | Loss: 1.3735\n",
      "Epoch: 0001/0100 | Batch 0950/1079 | Loss: 1.2432\n",
      "Epoch: 0001/0100 | Batch 1000/1079 | Loss: 1.3208\n",
      "Epoch: 0001/0100 | Batch 1050/1079 | Loss: 1.3733\n",
      "Training metrics: {'accuracy': 0.4137990962808481, 'f1': 0.25015078010182135}%\n",
      "Valid metrics: {'accuracy': 0.40906722251172484, 'f1': 0.25119140516357386}%\n",
      "Time elapsed: 22.05 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002/0100 | Batch 0000/1079 | Loss: 1.4736\n",
      "Epoch: 0002/0100 | Batch 0050/1079 | Loss: 1.3340\n",
      "Epoch: 0002/0100 | Batch 0100/1079 | Loss: 1.2751\n",
      "Epoch: 0002/0100 | Batch 0150/1079 | Loss: 1.2036\n",
      "Epoch: 0002/0100 | Batch 0200/1079 | Loss: 1.3686\n",
      "Epoch: 0002/0100 | Batch 0250/1079 | Loss: 1.3449\n",
      "Epoch: 0002/0100 | Batch 0300/1079 | Loss: 1.1732\n",
      "Epoch: 0002/0100 | Batch 0350/1079 | Loss: 1.3347\n",
      "Epoch: 0002/0100 | Batch 0400/1079 | Loss: 1.2917\n",
      "Epoch: 0002/0100 | Batch 0450/1079 | Loss: 1.5500\n",
      "Epoch: 0002/0100 | Batch 0500/1079 | Loss: 1.3027\n",
      "Epoch: 0002/0100 | Batch 0550/1079 | Loss: 1.2749\n",
      "Epoch: 0002/0100 | Batch 0600/1079 | Loss: 1.6974\n",
      "Epoch: 0002/0100 | Batch 0650/1079 | Loss: 1.3625\n",
      "Epoch: 0002/0100 | Batch 0700/1079 | Loss: 1.2334\n",
      "Epoch: 0002/0100 | Batch 0750/1079 | Loss: 1.4139\n",
      "Epoch: 0002/0100 | Batch 0800/1079 | Loss: 1.2639\n",
      "Epoch: 0002/0100 | Batch 0850/1079 | Loss: 1.5293\n",
      "Epoch: 0002/0100 | Batch 0900/1079 | Loss: 1.1033\n",
      "Epoch: 0002/0100 | Batch 0950/1079 | Loss: 1.2402\n",
      "Epoch: 0002/0100 | Batch 1000/1079 | Loss: 1.3338\n",
      "Epoch: 0002/0100 | Batch 1050/1079 | Loss: 1.1922\n",
      "Training metrics: {'accuracy': 0.44907890163364617, 'f1': 0.3198796588187166}%\n",
      "Valid metrics: {'accuracy': 0.4507556018759771, 'f1': 0.3154875942740484}%\n",
      "Time elapsed: 43.97 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0003/0100 | Batch 0000/1079 | Loss: 1.3334\n",
      "Epoch: 0003/0100 | Batch 0050/1079 | Loss: 1.1631\n",
      "Epoch: 0003/0100 | Batch 0100/1079 | Loss: 1.1263\n",
      "Epoch: 0003/0100 | Batch 0150/1079 | Loss: 1.2818\n",
      "Epoch: 0003/0100 | Batch 0200/1079 | Loss: 1.3980\n",
      "Epoch: 0003/0100 | Batch 0250/1079 | Loss: 1.3213\n",
      "Epoch: 0003/0100 | Batch 0300/1079 | Loss: 1.3912\n",
      "Epoch: 0003/0100 | Batch 0350/1079 | Loss: 1.3200\n",
      "Epoch: 0003/0100 | Batch 0400/1079 | Loss: 1.3359\n",
      "Epoch: 0003/0100 | Batch 0450/1079 | Loss: 1.2214\n",
      "Epoch: 0003/0100 | Batch 0500/1079 | Loss: 1.5438\n",
      "Epoch: 0003/0100 | Batch 0550/1079 | Loss: 1.3795\n",
      "Epoch: 0003/0100 | Batch 0600/1079 | Loss: 1.5804\n",
      "Epoch: 0003/0100 | Batch 0650/1079 | Loss: 1.4827\n",
      "Epoch: 0003/0100 | Batch 0700/1079 | Loss: 1.0985\n",
      "Epoch: 0003/0100 | Batch 0750/1079 | Loss: 1.0486\n",
      "Epoch: 0003/0100 | Batch 0800/1079 | Loss: 1.1911\n",
      "Epoch: 0003/0100 | Batch 0850/1079 | Loss: 1.2790\n",
      "Epoch: 0003/0100 | Batch 0900/1079 | Loss: 1.2670\n",
      "Epoch: 0003/0100 | Batch 0950/1079 | Loss: 1.4424\n",
      "Epoch: 0003/0100 | Batch 1000/1079 | Loss: 1.0865\n",
      "Epoch: 0003/0100 | Batch 1050/1079 | Loss: 1.4651\n",
      "Training metrics: {'accuracy': 0.45794230100799443, 'f1': 0.3613748916153172}%\n",
      "Valid metrics: {'accuracy': 0.4637832204273059, 'f1': 0.36251520224154443}%\n",
      "Time elapsed: 65.91 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0004/0100 | Batch 0000/1079 | Loss: 1.2360\n",
      "Epoch: 0004/0100 | Batch 0050/1079 | Loss: 1.3883\n",
      "Epoch: 0004/0100 | Batch 0100/1079 | Loss: 1.3417\n",
      "Epoch: 0004/0100 | Batch 0150/1079 | Loss: 1.2971\n",
      "Epoch: 0004/0100 | Batch 0200/1079 | Loss: 1.2402\n",
      "Epoch: 0004/0100 | Batch 0250/1079 | Loss: 1.3491\n",
      "Epoch: 0004/0100 | Batch 0300/1079 | Loss: 1.3716\n",
      "Epoch: 0004/0100 | Batch 0350/1079 | Loss: 1.3475\n",
      "Epoch: 0004/0100 | Batch 0400/1079 | Loss: 1.4718\n",
      "Epoch: 0004/0100 | Batch 0450/1079 | Loss: 1.2090\n",
      "Epoch: 0004/0100 | Batch 0500/1079 | Loss: 1.5349\n",
      "Epoch: 0004/0100 | Batch 0550/1079 | Loss: 1.3738\n",
      "Epoch: 0004/0100 | Batch 0600/1079 | Loss: 1.3432\n",
      "Epoch: 0004/0100 | Batch 0650/1079 | Loss: 1.3861\n",
      "Epoch: 0004/0100 | Batch 0700/1079 | Loss: 1.3107\n",
      "Epoch: 0004/0100 | Batch 0750/1079 | Loss: 1.1243\n",
      "Epoch: 0004/0100 | Batch 0800/1079 | Loss: 1.0966\n",
      "Epoch: 0004/0100 | Batch 0850/1079 | Loss: 1.3035\n",
      "Epoch: 0004/0100 | Batch 0900/1079 | Loss: 1.3273\n",
      "Epoch: 0004/0100 | Batch 0950/1079 | Loss: 1.2107\n",
      "Epoch: 0004/0100 | Batch 1000/1079 | Loss: 1.0709\n",
      "Epoch: 0004/0100 | Batch 1050/1079 | Loss: 1.1957\n",
      "Training metrics: {'accuracy': 0.4589271231606998, 'f1': 0.34187376043064965}%\n",
      "Valid metrics: {'accuracy': 0.4632621156852527, 'f1': 0.3471732013052101}%\n",
      "Time elapsed: 87.87 min\n",
      "Unfreezing the internal layers.\n",
      "Optimizer and scheduler are reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005/0100 | Batch 0000/1079 | Loss: 1.2428\n",
      "Epoch: 0005/0100 | Batch 0050/1079 | Loss: 1.2353\n",
      "Epoch: 0005/0100 | Batch 0100/1079 | Loss: 1.1145\n",
      "Epoch: 0005/0100 | Batch 0150/1079 | Loss: 1.3703\n",
      "Epoch: 0005/0100 | Batch 0200/1079 | Loss: 1.1818\n",
      "Epoch: 0005/0100 | Batch 0250/1079 | Loss: 0.9971\n",
      "Epoch: 0005/0100 | Batch 0300/1079 | Loss: 1.1260\n",
      "Epoch: 0005/0100 | Batch 0350/1079 | Loss: 1.3950\n",
      "Epoch: 0005/0100 | Batch 0400/1079 | Loss: 1.4524\n",
      "Epoch: 0005/0100 | Batch 0450/1079 | Loss: 1.1353\n",
      "Epoch: 0005/0100 | Batch 0500/1079 | Loss: 1.4976\n",
      "Epoch: 0005/0100 | Batch 0550/1079 | Loss: 1.1397\n",
      "Epoch: 0005/0100 | Batch 0600/1079 | Loss: 1.2999\n",
      "Epoch: 0005/0100 | Batch 0650/1079 | Loss: 1.1949\n",
      "Epoch: 0005/0100 | Batch 0700/1079 | Loss: 1.1962\n",
      "Epoch: 0005/0100 | Batch 0750/1079 | Loss: 0.8304\n",
      "Epoch: 0005/0100 | Batch 0800/1079 | Loss: 1.6045\n",
      "Epoch: 0005/0100 | Batch 0850/1079 | Loss: 1.7314\n",
      "Epoch: 0005/0100 | Batch 0900/1079 | Loss: 1.3485\n",
      "Epoch: 0005/0100 | Batch 0950/1079 | Loss: 1.1982\n",
      "Epoch: 0005/0100 | Batch 1000/1079 | Loss: 0.9710\n",
      "Epoch: 0005/0100 | Batch 1050/1079 | Loss: 0.9771\n",
      "Training metrics: {'accuracy': 0.5401459854014599, 'f1': 0.45824630210399936}%\n",
      "Valid metrics: {'accuracy': 0.52892131318395, 'f1': 0.44879653215364}%\n",
      "Time elapsed: 119.92 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0006/0100 | Batch 0000/1079 | Loss: 0.9895\n",
      "Epoch: 0006/0100 | Batch 0050/1079 | Loss: 1.3431\n",
      "Epoch: 0006/0100 | Batch 0100/1079 | Loss: 0.8798\n",
      "Epoch: 0006/0100 | Batch 0150/1079 | Loss: 1.0577\n",
      "Epoch: 0006/0100 | Batch 0200/1079 | Loss: 0.9373\n",
      "Epoch: 0006/0100 | Batch 0250/1079 | Loss: 1.2596\n",
      "Epoch: 0006/0100 | Batch 0300/1079 | Loss: 1.4112\n",
      "Epoch: 0006/0100 | Batch 0350/1079 | Loss: 0.8772\n",
      "Epoch: 0006/0100 | Batch 0400/1079 | Loss: 0.8310\n",
      "Epoch: 0006/0100 | Batch 0450/1079 | Loss: 1.2238\n",
      "Epoch: 0006/0100 | Batch 0500/1079 | Loss: 1.4033\n",
      "Epoch: 0006/0100 | Batch 0550/1079 | Loss: 1.4515\n",
      "Epoch: 0006/0100 | Batch 0600/1079 | Loss: 1.4788\n",
      "Epoch: 0006/0100 | Batch 0650/1079 | Loss: 0.9864\n",
      "Epoch: 0006/0100 | Batch 0700/1079 | Loss: 0.9317\n",
      "Epoch: 0006/0100 | Batch 0750/1079 | Loss: 0.9378\n",
      "Epoch: 0006/0100 | Batch 0800/1079 | Loss: 1.0006\n",
      "Epoch: 0006/0100 | Batch 0850/1079 | Loss: 0.8337\n",
      "Epoch: 0006/0100 | Batch 0900/1079 | Loss: 1.3156\n",
      "Epoch: 0006/0100 | Batch 0950/1079 | Loss: 1.2576\n",
      "Epoch: 0006/0100 | Batch 1000/1079 | Loss: 0.8628\n",
      "Epoch: 0006/0100 | Batch 1050/1079 | Loss: 1.3287\n",
      "Training metrics: {'accuracy': 0.583709882979956, 'f1': 0.5298001516703283}%\n",
      "Valid metrics: {'accuracy': 0.5581031787389266, 'f1': 0.49853224554853953}%\n",
      "Time elapsed: 151.99 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0007/0100 | Batch 0000/1079 | Loss: 0.8725\n",
      "Epoch: 0007/0100 | Batch 0050/1079 | Loss: 0.6471\n",
      "Epoch: 0007/0100 | Batch 0100/1079 | Loss: 0.8418\n",
      "Epoch: 0007/0100 | Batch 0150/1079 | Loss: 1.3544\n",
      "Epoch: 0007/0100 | Batch 0200/1079 | Loss: 0.9458\n",
      "Epoch: 0007/0100 | Batch 0250/1079 | Loss: 0.8585\n",
      "Epoch: 0007/0100 | Batch 0300/1079 | Loss: 1.3754\n",
      "Epoch: 0007/0100 | Batch 0350/1079 | Loss: 1.0617\n",
      "Epoch: 0007/0100 | Batch 0400/1079 | Loss: 1.3040\n",
      "Epoch: 0007/0100 | Batch 0450/1079 | Loss: 2.0273\n",
      "Epoch: 0007/0100 | Batch 0500/1079 | Loss: 1.2222\n",
      "Epoch: 0007/0100 | Batch 0550/1079 | Loss: 0.9451\n",
      "Epoch: 0007/0100 | Batch 0600/1079 | Loss: 1.2362\n",
      "Epoch: 0007/0100 | Batch 0650/1079 | Loss: 1.0123\n",
      "Epoch: 0007/0100 | Batch 0700/1079 | Loss: 0.8055\n",
      "Epoch: 0007/0100 | Batch 0750/1079 | Loss: 0.7398\n",
      "Epoch: 0007/0100 | Batch 0800/1079 | Loss: 1.1428\n",
      "Epoch: 0007/0100 | Batch 0850/1079 | Loss: 1.2208\n",
      "Epoch: 0007/0100 | Batch 0900/1079 | Loss: 1.2469\n",
      "Epoch: 0007/0100 | Batch 0950/1079 | Loss: 0.8186\n",
      "Epoch: 0007/0100 | Batch 1000/1079 | Loss: 0.9879\n",
      "Epoch: 0007/0100 | Batch 1050/1079 | Loss: 1.0276\n",
      "Training metrics: {'accuracy': 0.5987718688448616, 'f1': 0.5174521939571427}%\n",
      "Valid metrics: {'accuracy': 0.5643564356435643, 'f1': 0.47212462297462005}%\n",
      "Time elapsed: 183.95 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0008/0100 | Batch 0000/1079 | Loss: 1.4612\n",
      "Epoch: 0008/0100 | Batch 0050/1079 | Loss: 1.0421\n",
      "Epoch: 0008/0100 | Batch 0100/1079 | Loss: 0.8888\n",
      "Epoch: 0008/0100 | Batch 0150/1079 | Loss: 0.9538\n",
      "Epoch: 0008/0100 | Batch 0200/1079 | Loss: 0.7105\n",
      "Epoch: 0008/0100 | Batch 0250/1079 | Loss: 0.7820\n",
      "Epoch: 0008/0100 | Batch 0300/1079 | Loss: 1.0625\n",
      "Epoch: 0008/0100 | Batch 0350/1079 | Loss: 0.8905\n",
      "Epoch: 0008/0100 | Batch 0400/1079 | Loss: 1.0420\n",
      "Epoch: 0008/0100 | Batch 0450/1079 | Loss: 0.9520\n",
      "Epoch: 0008/0100 | Batch 0500/1079 | Loss: 0.8210\n",
      "Epoch: 0008/0100 | Batch 0550/1079 | Loss: 1.1957\n",
      "Epoch: 0008/0100 | Batch 0600/1079 | Loss: 0.8798\n",
      "Epoch: 0008/0100 | Batch 0650/1079 | Loss: 1.3651\n",
      "Epoch: 0008/0100 | Batch 0700/1079 | Loss: 1.2335\n",
      "Epoch: 0008/0100 | Batch 0750/1079 | Loss: 0.8597\n",
      "Epoch: 0008/0100 | Batch 0800/1079 | Loss: 0.9508\n",
      "Epoch: 0008/0100 | Batch 0850/1079 | Loss: 0.8525\n",
      "Epoch: 0008/0100 | Batch 0900/1079 | Loss: 0.9042\n",
      "Epoch: 0008/0100 | Batch 0950/1079 | Loss: 1.0948\n",
      "Epoch: 0008/0100 | Batch 1000/1079 | Loss: 0.8762\n",
      "Epoch: 0008/0100 | Batch 1050/1079 | Loss: 0.9921\n",
      "Training metrics: {'accuracy': 0.6118642104043563, 'f1': 0.5898664284030974}%\n",
      "Valid metrics: {'accuracy': 0.5539343408025013, 'f1': 0.5207451239968588}%\n",
      "Time elapsed: 216.00 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0009/0100 | Batch 0000/1079 | Loss: 0.9810\n",
      "Epoch: 0009/0100 | Batch 0050/1079 | Loss: 0.8935\n",
      "Epoch: 0009/0100 | Batch 0100/1079 | Loss: 0.9552\n",
      "Epoch: 0009/0100 | Batch 0150/1079 | Loss: 0.9739\n",
      "Epoch: 0009/0100 | Batch 0200/1079 | Loss: 0.6786\n",
      "Epoch: 0009/0100 | Batch 0250/1079 | Loss: 1.1057\n",
      "Epoch: 0009/0100 | Batch 0300/1079 | Loss: 1.1765\n",
      "Epoch: 0009/0100 | Batch 0350/1079 | Loss: 1.0007\n",
      "Epoch: 0009/0100 | Batch 0400/1079 | Loss: 1.0148\n",
      "Epoch: 0009/0100 | Batch 0450/1079 | Loss: 0.8195\n",
      "Epoch: 0009/0100 | Batch 0500/1079 | Loss: 0.9676\n",
      "Epoch: 0009/0100 | Batch 0550/1079 | Loss: 0.6957\n",
      "Epoch: 0009/0100 | Batch 0600/1079 | Loss: 1.0698\n",
      "Epoch: 0009/0100 | Batch 0650/1079 | Loss: 0.9683\n",
      "Epoch: 0009/0100 | Batch 0700/1079 | Loss: 0.7005\n",
      "Epoch: 0009/0100 | Batch 0750/1079 | Loss: 1.1549\n",
      "Epoch: 0009/0100 | Batch 0800/1079 | Loss: 1.0840\n",
      "Epoch: 0009/0100 | Batch 0850/1079 | Loss: 1.1707\n",
      "Epoch: 0009/0100 | Batch 0900/1079 | Loss: 0.6795\n",
      "Epoch: 0009/0100 | Batch 0950/1079 | Loss: 0.6739\n",
      "Epoch: 0009/0100 | Batch 1000/1079 | Loss: 1.1661\n",
      "Epoch: 0009/0100 | Batch 1050/1079 | Loss: 0.6038\n",
      "Training metrics: {'accuracy': 0.6343992584868497, 'f1': 0.6325599345558964}%\n",
      "Valid metrics: {'accuracy': 0.5560187597707139, 'f1': 0.5445092049028138}%\n",
      "Time elapsed: 248.01 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010/0100 | Batch 0000/1079 | Loss: 1.1125\n",
      "Epoch: 0010/0100 | Batch 0050/1079 | Loss: 0.8207\n",
      "Epoch: 0010/0100 | Batch 0100/1079 | Loss: 0.9390\n",
      "Epoch: 0010/0100 | Batch 0150/1079 | Loss: 0.8335\n",
      "Epoch: 0010/0100 | Batch 0200/1079 | Loss: 0.7403\n",
      "Epoch: 0010/0100 | Batch 0250/1079 | Loss: 0.7540\n",
      "Epoch: 0010/0100 | Batch 0300/1079 | Loss: 0.6373\n",
      "Epoch: 0010/0100 | Batch 0350/1079 | Loss: 0.6400\n",
      "Epoch: 0010/0100 | Batch 0400/1079 | Loss: 0.7170\n",
      "Epoch: 0010/0100 | Batch 0450/1079 | Loss: 1.2640\n",
      "Epoch: 0010/0100 | Batch 0500/1079 | Loss: 0.3986\n",
      "Epoch: 0010/0100 | Batch 0550/1079 | Loss: 0.8150\n",
      "Epoch: 0010/0100 | Batch 0600/1079 | Loss: 1.0462\n",
      "Epoch: 0010/0100 | Batch 0650/1079 | Loss: 0.8416\n",
      "Epoch: 0010/0100 | Batch 0700/1079 | Loss: 0.5135\n",
      "Epoch: 0010/0100 | Batch 0750/1079 | Loss: 1.0651\n",
      "Epoch: 0010/0100 | Batch 0800/1079 | Loss: 0.6235\n",
      "Epoch: 0010/0100 | Batch 0850/1079 | Loss: 1.0132\n",
      "Epoch: 0010/0100 | Batch 0900/1079 | Loss: 0.9822\n",
      "Epoch: 0010/0100 | Batch 0950/1079 | Loss: 1.0965\n",
      "Epoch: 0010/0100 | Batch 1000/1079 | Loss: 0.8520\n",
      "Epoch: 0010/0100 | Batch 1050/1079 | Loss: 0.6818\n",
      "Training metrics: {'accuracy': 0.7444676167303904, 'f1': 0.7242665651031835}%\n",
      "Valid metrics: {'accuracy': 0.6112558624283481, 'f1': 0.5753494011773091}%\n",
      "Time elapsed: 280.21 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0011/0100 | Batch 0000/1079 | Loss: 0.8565\n",
      "Epoch: 0011/0100 | Batch 0050/1079 | Loss: 0.4793\n",
      "Epoch: 0011/0100 | Batch 0100/1079 | Loss: 0.6685\n",
      "Epoch: 0011/0100 | Batch 0150/1079 | Loss: 0.5238\n",
      "Epoch: 0011/0100 | Batch 0200/1079 | Loss: 1.1236\n",
      "Epoch: 0011/0100 | Batch 0250/1079 | Loss: 0.6991\n",
      "Epoch: 0011/0100 | Batch 0300/1079 | Loss: 0.5967\n",
      "Epoch: 0011/0100 | Batch 0350/1079 | Loss: 0.7529\n",
      "Epoch: 0011/0100 | Batch 0400/1079 | Loss: 0.7039\n",
      "Epoch: 0011/0100 | Batch 0450/1079 | Loss: 0.5951\n",
      "Epoch: 0011/0100 | Batch 0500/1079 | Loss: 0.5359\n",
      "Epoch: 0011/0100 | Batch 0550/1079 | Loss: 0.7180\n",
      "Epoch: 0011/0100 | Batch 0600/1079 | Loss: 0.6007\n",
      "Epoch: 0011/0100 | Batch 0650/1079 | Loss: 1.1209\n",
      "Epoch: 0011/0100 | Batch 0700/1079 | Loss: 1.0081\n",
      "Epoch: 0011/0100 | Batch 0750/1079 | Loss: 0.5195\n",
      "Epoch: 0011/0100 | Batch 0800/1079 | Loss: 0.5510\n",
      "Epoch: 0011/0100 | Batch 0850/1079 | Loss: 0.7092\n",
      "Epoch: 0011/0100 | Batch 0900/1079 | Loss: 0.7039\n",
      "Epoch: 0011/0100 | Batch 0950/1079 | Loss: 0.8103\n",
      "Epoch: 0011/0100 | Batch 1000/1079 | Loss: 0.7462\n",
      "Epoch: 0011/0100 | Batch 1050/1079 | Loss: 0.6602\n",
      "Training metrics: {'accuracy': 0.7807322442358938, 'f1': 0.7697715428668136}%\n",
      "Valid metrics: {'accuracy': 0.6065659197498697, 'f1': 0.5815988529691516}%\n",
      "Time elapsed: 312.24 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0012/0100 | Batch 0000/1079 | Loss: 0.5369\n",
      "Epoch: 0012/0100 | Batch 0050/1079 | Loss: 0.7127\n",
      "Epoch: 0012/0100 | Batch 0100/1079 | Loss: 0.5432\n",
      "Epoch: 0012/0100 | Batch 0150/1079 | Loss: 0.6468\n",
      "Epoch: 0012/0100 | Batch 0200/1079 | Loss: 0.4184\n",
      "Epoch: 0012/0100 | Batch 0250/1079 | Loss: 0.4716\n",
      "Epoch: 0012/0100 | Batch 0300/1079 | Loss: 0.8530\n",
      "Epoch: 0012/0100 | Batch 0350/1079 | Loss: 0.5283\n",
      "Epoch: 0012/0100 | Batch 0400/1079 | Loss: 1.1281\n",
      "Epoch: 0012/0100 | Batch 0450/1079 | Loss: 0.5559\n",
      "Epoch: 0012/0100 | Batch 0500/1079 | Loss: 0.7871\n",
      "Epoch: 0012/0100 | Batch 0550/1079 | Loss: 0.8743\n",
      "Epoch: 0012/0100 | Batch 0600/1079 | Loss: 0.5598\n",
      "Epoch: 0012/0100 | Batch 0650/1079 | Loss: 0.6864\n",
      "Epoch: 0012/0100 | Batch 0700/1079 | Loss: 0.8918\n",
      "Epoch: 0012/0100 | Batch 0750/1079 | Loss: 0.6095\n",
      "Epoch: 0012/0100 | Batch 0800/1079 | Loss: 0.6292\n",
      "Epoch: 0012/0100 | Batch 0850/1079 | Loss: 0.6359\n",
      "Epoch: 0012/0100 | Batch 0900/1079 | Loss: 0.5284\n",
      "Epoch: 0012/0100 | Batch 0950/1079 | Loss: 0.9516\n",
      "Epoch: 0012/0100 | Batch 1000/1079 | Loss: 0.6855\n",
      "Epoch: 0012/0100 | Batch 1050/1079 | Loss: 0.6794\n",
      "Training metrics: {'accuracy': 0.7874522071602363, 'f1': 0.7707239839077421}%\n",
      "Valid metrics: {'accuracy': 0.5997915581031787, 'f1': 0.5719384024958922}%\n",
      "Time elapsed: 344.31 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0013/0100 | Batch 0000/1079 | Loss: 0.4223\n",
      "Epoch: 0013/0100 | Batch 0050/1079 | Loss: 0.3904\n",
      "Epoch: 0013/0100 | Batch 0100/1079 | Loss: 0.6942\n",
      "Epoch: 0013/0100 | Batch 0150/1079 | Loss: 0.8111\n",
      "Epoch: 0013/0100 | Batch 0200/1079 | Loss: 0.8359\n",
      "Epoch: 0013/0100 | Batch 0250/1079 | Loss: 0.9845\n",
      "Epoch: 0013/0100 | Batch 0300/1079 | Loss: 0.5932\n",
      "Epoch: 0013/0100 | Batch 0350/1079 | Loss: 0.3291\n",
      "Epoch: 0013/0100 | Batch 0400/1079 | Loss: 0.5883\n",
      "Epoch: 0013/0100 | Batch 0450/1079 | Loss: 0.5227\n",
      "Epoch: 0013/0100 | Batch 0500/1079 | Loss: 0.9105\n",
      "Epoch: 0013/0100 | Batch 0550/1079 | Loss: 0.5423\n",
      "Epoch: 0013/0100 | Batch 0600/1079 | Loss: 0.7736\n",
      "Epoch: 0013/0100 | Batch 0650/1079 | Loss: 0.6702\n",
      "Epoch: 0013/0100 | Batch 0700/1079 | Loss: 0.6926\n",
      "Epoch: 0013/0100 | Batch 0750/1079 | Loss: 0.4628\n",
      "Epoch: 0013/0100 | Batch 0800/1079 | Loss: 0.8514\n",
      "Epoch: 0013/0100 | Batch 0850/1079 | Loss: 0.4652\n",
      "Epoch: 0013/0100 | Batch 0900/1079 | Loss: 0.2387\n",
      "Epoch: 0013/0100 | Batch 0950/1079 | Loss: 1.0069\n",
      "Epoch: 0013/0100 | Batch 1000/1079 | Loss: 0.5891\n",
      "Epoch: 0013/0100 | Batch 1050/1079 | Loss: 0.9065\n",
      "Training metrics: {'accuracy': 0.8111458695400301, 'f1': 0.8064948390721363}%\n",
      "Valid metrics: {'accuracy': 0.5810317873892652, 'f1': 0.5570119829603672}%\n",
      "Time elapsed: 376.32 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g100/home/userexternal/mhabibi0/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0014/0100 | Batch 0000/1079 | Loss: 0.6850\n",
      "Epoch: 0014/0100 | Batch 0050/1079 | Loss: 0.8247\n",
      "Epoch: 0014/0100 | Batch 0100/1079 | Loss: 0.6380\n",
      "Epoch: 0014/0100 | Batch 0150/1079 | Loss: 0.6657\n",
      "Epoch: 0014/0100 | Batch 0200/1079 | Loss: 0.5309\n",
      "Epoch: 0014/0100 | Batch 0250/1079 | Loss: 0.2366\n",
      "Epoch: 0014/0100 | Batch 0300/1079 | Loss: 0.4294\n",
      "Epoch: 0014/0100 | Batch 0350/1079 | Loss: 0.4415\n",
      "Epoch: 0014/0100 | Batch 0400/1079 | Loss: 0.7213\n",
      "Epoch: 0014/0100 | Batch 0450/1079 | Loss: 0.4210\n",
      "Epoch: 0014/0100 | Batch 0500/1079 | Loss: 0.4092\n",
      "Epoch: 0014/0100 | Batch 0550/1079 | Loss: 0.3034\n",
      "Epoch: 0014/0100 | Batch 0600/1079 | Loss: 0.4706\n",
      "Epoch: 0014/0100 | Batch 0650/1079 | Loss: 0.5994\n",
      "Epoch: 0014/0100 | Batch 0700/1079 | Loss: 0.5055\n",
      "Epoch: 0014/0100 | Batch 0750/1079 | Loss: 0.5773\n",
      "Epoch: 0014/0100 | Batch 0800/1079 | Loss: 0.8204\n",
      "Epoch: 0014/0100 | Batch 0850/1079 | Loss: 0.6365\n",
      "Epoch: 0014/0100 | Batch 0900/1079 | Loss: 0.6776\n",
      "Epoch: 0014/0100 | Batch 0950/1079 | Loss: 0.4643\n",
      "Epoch: 0014/0100 | Batch 1000/1079 | Loss: 1.2217\n",
      "Epoch: 0014/0100 | Batch 1050/1079 | Loss: 1.2014\n",
      "Training metrics: {'accuracy': 0.7964314679643146, 'f1': 0.783095854809847}%\n",
      "Valid metrics: {'accuracy': 0.5862428348097968, 'f1': 0.5576612098278776}%\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start_time = time.time()\n",
    "\n",
    "# Freeze all layers except the classifier \n",
    "freeze_epochs = 4\n",
    "frozen = True\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name: # classifier layer\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# Initialize best accuracy and epochs since improvement\n",
    "best_f1 = 0.0\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "\n",
    "# opimizer\n",
    "LR_freezed = 8e-5\n",
    "LR_unfreezed = 4e-6\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=LR_freezed)\n",
    "\n",
    "# learning rate scheduler.\n",
    "scheduler = ReduceLROnPlateau(optim, mode='max', factor=0.2, patience=2)\n",
    "\n",
    "                \n",
    "                          \n",
    "NUM_EPOCHS = 100\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # Unfreeze all layers after 2 epochs\n",
    "    if epoch  == freeze_epochs:\n",
    "        print('Unfreezing the internal layers.')\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True   \n",
    "            \n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=LR_unfreezed)\n",
    "        scheduler = ReduceLROnPlateau(optim, mode='max', factor=0.2, patience=2)\n",
    "        \n",
    "        print('Optimizer and scheduler are reset')\n",
    "\n",
    "            \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        ### Logging\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        \n",
    "        print(f'Training metrics: '\n",
    "              f'{compute_metrics(model, train_loader, DEVICE)}%'\n",
    "              f'\\nValid metrics: '\n",
    "              f'{compute_metrics(model, valid_loader, DEVICE)}%')\n",
    "\n",
    "        current_f1 = compute_metrics(model, valid_loader, DEVICE)['f1']\n",
    "        \n",
    "        # step the scheduler ReduceLROnPlateau\n",
    "        scheduler.step(current_f1) \n",
    "        \n",
    "        # check if improved\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            epochs_since_improvement = 0\n",
    "            \n",
    "            # Save the new best model\n",
    "            path = os.path.join(model_dir ,'XLM2022_age_mentalism_extra_features.pt')\n",
    "            torch.save(model, path) \n",
    "                \n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if epochs_since_improvement >= 3:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "406e4c93-092a-4071-b32a-9cd06fe08bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save models in work directory\n",
    "# path = os.path.join(uc_dir, 'trained_models', 'mentalism')\n",
    "# if not os.path.exists(path):\n",
    "#     os.mkdir(path)\n",
    "    \n",
    "# sub_path = os.path.join(path, 'age')\n",
    "# if not os.path.exists(sub_path):\n",
    "#         os.mkdir(sub_path)\n",
    "\n",
    "# # XLM\n",
    "# import shutil\n",
    "# source_path =  os.path.join(model_dir ,'XLM_age_mentalism.pt')\n",
    "# dest_path = os.path.join(sub_path, 'XLM_age.pt')\n",
    "# shutil.copy(source_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6f3f5-8046-462e-992e-06ffbe908a3b",
   "metadata": {},
   "source": [
    "## Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f15b1219-6ac5-4fea-b6fa-7641f85b0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user age data\n",
    "path  = os.path.join(uc_dir, 'data_for_models_test.pkl')\n",
    "df_test = pd.read_pickle(path)\n",
    "\n",
    "# Discretize the 'age' column into five classes\n",
    "age_intervals = [0, 18, 30, 40, 60, 100]\n",
    "age_labels = [0, 1, 2, 3, 4]\n",
    "df = df_test[df_test['age']<=99]\n",
    "df_test['age_class'] = pd.cut(df_test['age'], bins=age_intervals, labels=age_labels, right=False).astype(int)\n",
    "\n",
    "\n",
    "df_test['text']  = 'bio: ' + df_test['masked_bio'] + '. ' + 'tweets: ' + df_test['long_text'] \n",
    "df_test['text'] = df_test['text'].str.replace('\\r|\\n', ' ', regex=True)\n",
    "\n",
    "X_test = df_test['text'].values\n",
    "y_test = df_test['age_class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25f07b1c-b5bf-48ee-bfc0-d5bbc063e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-large-2022\")\n",
    "\n",
    "# test encodings and dataset\n",
    "xlm_test_encodings = batch_tokenize(X_test, tokenizer)\n",
    "\n",
    "xlm_test_dataset = TweetDataset(xlm_test_encodings, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3135562-c859-4aab-8a11-315b4f5a1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "xlm_loader = torch.utils.data.DataLoader(\n",
    "    xlm_test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c8adee-3c81-4988-a7ef-bcff5a06d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model_path, data_loader, device):\n",
    "    \n",
    "    # Load the saved model\n",
    "    model = torch.load(model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # store predicted probs\n",
    "    class_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            class_probs.extend(probabilities.cpu().numpy().tolist())\n",
    "\n",
    "            \n",
    "    # Compute the metrics\n",
    "    metrics = compute_metrics(model, data_loader, device)\n",
    "    \n",
    "    return metrics, np.array(class_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63052647-f123-40b9-acfc-4218c0e5e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM\n",
    "path = os.path.join(model_dir ,'XLM2022_age_mentalism_extra_features.pt')\n",
    "metrics, xlm_probs = test_eval(path, xlm_loader, DEVICE)\n",
    "\n",
    "# # save probs\n",
    "# path = os.path.join(uc_dir, 'trained_models', 'age', 'XLM_probs_age.npy')\n",
    "# np.save(path, xlm_probs)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343d171-336d-47c7-8cb7-8d75e0609ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 1.12.1 (Python 3.10)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
